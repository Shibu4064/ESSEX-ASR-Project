{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":6713857,"datasetId":3868867,"databundleVersionId":6798290},{"sourceType":"datasetVersion","sourceId":14707507,"datasetId":9396421,"databundleVersionId":15552869}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"67522d9f-7ed9-4f36-b2e9-2a416acc937f","cell_type":"markdown","source":"# Cross‑Domain ASR on Kaggle P100 (Whisper + LoRA)  \n**Datasets:** LibriSpeech (train-clean-360/dev-clean/test-clean) + Mozilla Common Voice v24 en‑AU (local folder)  \n**Goal:** improve **WER/CER** under Kaggle P100 limits using a high‑ROI recipe:\n\n1) LoRA + 8‑bit optimizer  \n2) Quality‑aware filtering / curriculum  \n3) Speaker‑balanced sampling  \n4) SpecAugment  \n5) Two‑stage training (LibriSpeech → Common Voice adaptation)  \n6) Confidence‑based transcript denoising  \n7) Decode‑time tuning (beam/penalties/temperature)\n\n> Run top‑to‑bottom; use `CFG` switches for ablations.\n","metadata":{}},{"id":"18b5fd66-1b7e-48ca-b7de-bc0ee8394ca2","cell_type":"markdown","source":"## 0) Imports + Reproducibility + GPU sanity","metadata":{}},{"id":"d4074577-8d4c-469a-a67c-29e250f693a3","cell_type":"code","source":"# Cell 1 — P100-safe environment (fixes: torchvision mismatch, bitsandbytes/triton, and sm_60 kernels)\n\n# IMPORTANT:\n# - Tesla P100 = compute capability sm_60\n# - PyTorch CUDA 12.8/12.9 wheels may DROP sm_60 support → \"no kernel image\" errors.\n# - Use CUDA 12.6 (cu126) wheels for torch/torchaudio on Kaggle P100.\n\n# Clean potentially conflicting packages\n!pip -q uninstall -y torch torchvision torchaudio triton bitsandbytes transformers tokenizers accelerate peft evaluate datasets\n\n# Install PyTorch CUDA 12.6 wheels (P100-compatible)\n# NOTE: After this cell finishes, restart the Kaggle session once (Runtime → Restart session),\n# then continue from Cell 2.\n!pip -q install --no-cache-dir --index-url https://download.pytorch.org/whl/cu126 \\\n  torch==2.8.0+cu126 torchaudio==2.8.0+cu126\n\n# Install ASR stack (no bitsandbytes; torchvision not required)\n!pip -q install --no-cache-dir \\\n  transformers==4.52.1 \\\n  datasets==2.20.0 \\\n  accelerate==0.34.2 \\\n  evaluate==0.4.2 \\\n  peft==0.11.1 \\\n  jiwer librosa soundfile\n\nprint(\"✅ Install complete. NOW restart the session once, then run Cell 2.\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:35:37.394915Z","iopub.execute_input":"2026-02-19T23:35:37.395241Z","iopub.status.idle":"2026-02-19T23:36:37.109135Z","shell.execute_reply.started":"2026-02-19T23:35:37.395198Z","shell.execute_reply":"2026-02-19T23:36:37.108308Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.8/821.8 MB\u001b[0m \u001b[31m302.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m252.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\ntimm 1.0.20 requires torchvision, which is not installed.\nfastai 2.8.4 requires torchvision>=0.11, which is not installed.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m303.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m262.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m176.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m306.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m263.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h✅ Install complete. NOW restart the session once, then run Cell 2.\n","output_type":"stream"}],"execution_count":1},{"id":"d66d3bea-0afd-4fc4-9ff3-0fe19dbcc5f7","cell_type":"code","source":"# Cell X — P100 runtime sanity + cache locations (run before importing transformers/datasets)\n\nimport os, torch\n\n# Avoid accidental torchvision imports inside transformers\nos.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Keep ALL caches inside /kaggle/working (counts toward 19.5GB limit, but avoids filling /root)\nHF_BASE = \"/kaggle/working/hf_cache\"\nos.environ[\"HF_HOME\"] = HF_BASE\nos.environ[\"HF_DATASETS_CACHE\"] = os.path.join(HF_BASE, \"datasets\")\nos.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(HF_BASE, \"transformers\")\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\n# Optional: better allocator behavior (helps stability on long runs)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# P100-safe attention backend:\n# Disable Flash + mem-efficient SDPA kernels; force math SDPA\nif torch.cuda.is_available():\n    try:\n        torch.backends.cuda.enable_flash_sdp(False)\n        torch.backends.cuda.enable_mem_efficient_sdp(False)\n        torch.backends.cuda.enable_math_sdp(True)\n    except Exception as e:\n        print(\"SDPA backend tweak skipped:\", e)\n\nprint(\"torch:\", torch.__version__, \"| cuda:\", torch.version.cuda)\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0), \"| capability:\", torch.cuda.get_device_capability(0))\n    print(\"torch arch list:\", torch.cuda.get_arch_list())\n    # quick kernel sanity test\n    _ = torch.zeros((2,3), device=\"cuda\")\n    print(\"✅ CUDA kernel sanity test passed\")\nelse:\n    print(\"⚠️ CUDA not available\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:36:37.111342Z","iopub.execute_input":"2026-02-19T23:36:37.111602Z","iopub.status.idle":"2026-02-19T23:36:40.315586Z","shell.execute_reply.started":"2026-02-19T23:36:37.111573Z","shell.execute_reply":"2026-02-19T23:36:40.314783Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch: 2.8.0+cu126 | cuda: 12.6\nGPU: Tesla P100-PCIE-16GB | capability: (6, 0)\ntorch arch list: ['sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90']\n✅ CUDA kernel sanity test passed\n","output_type":"stream"}],"execution_count":2},{"id":"335f5797-47f8-4224-9aea-c86a49cc1f27","cell_type":"code","source":"# Cell 2 — Imports and basic setup\n\nimport os, re, math, json, random\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torchaudio\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    set_seed\n)\n\nfrom jiwer import wer, cer\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n\nSEED = 42\nset_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:36:51.030082Z","iopub.execute_input":"2026-02-19T23:36:51.030416Z","iopub.status.idle":"2026-02-19T23:37:15.653989Z","shell.execute_reply.started":"2026-02-19T23:36:51.030387Z","shell.execute_reply":"2026-02-19T23:37:15.653277Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n2026-02-19 23:36:59.868964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771544220.072213     149 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771544220.127550     149 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771544220.632756     149 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544220.632842     149 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544220.632847     149 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544220.632850     149 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Torch: 2.8.0+cu126\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7be130fa4510>"},"metadata":{}}],"execution_count":3},{"id":"504b83a4-fd33-43ef-a3d9-d46e087bb26f","cell_type":"markdown","source":"## 1) Configuration (P100‑friendly defaults)","metadata":{}},{"id":"a939e240-8408-4911-8c0a-0d595850f7db","cell_type":"code","source":"# Cell 3 — Config switches and limits (edit these first)\n\nfrom pathlib import Path\n\nCFG = {\n    # Explicit dataset roots (NO auto-detection)\n    \"LIBRISPEECH_ROOT\": \"/kaggle/input/datasets/pypiahmad/librispeech-asr-corpus\",\n    \"CV_ROOT\": \"/kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice\",\n\n    # Common Voice structure inside CV_ROOT\n    \"CV_LANG_DIR\": \"commonvoice-v24_en-AU\",\n    \"CV_AUDIO_DIR\": \"audio_files\",\n    \"CV_MAIN_CSV\": \"commonvoice-v24_en-AU.csv\",\n    \"CV_SPLIT_CSV\": \"commonvoice-v24_en-AU-split.csv\",\n\n    # Model (newer)\n    \"MODEL_NAME\": \"openai/whisper-small.en\",\n\n    # Caps for iteration speed (set None for full data)\n    \"MAX_LS_TRAIN_ROWS\": 60000,\n    \"MAX_LS_DEV_ROWS\": 5000,\n    \"MAX_CV_TRAIN_ROWS\": 25000,\n    \"MAX_CV_DEV_ROWS\": 3000,\n    \"MAX_CV_TEST_ROWS\": 3000,\n\n    # Constraints\n    \"MIN_AUDIO_SEC\": 1.0,\n    \"MAX_AUDIO_SEC\": 20.0,\n    \"MAX_LABEL_CHARS\": 220,\n\n    # ROI feature toggles\n    \"USE_QUALITY_FILTERING\": True,\n    \"USE_CURRICULUM\": True,\n    \"USE_SPEAKER_BALANCED_SAMPLING\": True,\n    \"USE_SPECAUG\": True,\n    \"USE_LORA\": True,\n    \"USE_8BIT_OPTIM\": True,\n    \"USE_TRANSCRIPT_DENOISE\": True,\n    \"USE_DECODE_TUNING\": True,\n\n    \"QUALITY_METRIC_MAX_SAMPLES\": 12000,\n    \"DENOISE_SCORE_MAX_SAMPLES\": 8000,\n    \"DENOISE_DROP_FRACTION\": 0.12,\n\n    # Stage A (LibriSpeech)\n    \"STAGE_A_MAX_STEPS\": 1200,\n    \"STAGE_A_LR\": 1e-4,\n\n    # Stage B (Common Voice)\n    \"STAGE_B_MAX_STEPS\": 900,\n    \"STAGE_B_LR\": 7e-5,\n\n    # Batch sizing (P100-safe; you may need BS=2 for large-v3-turbo)\n    \"PER_DEVICE_TRAIN_BS\": 2,\n    \"PER_DEVICE_EVAL_BS\": 2,\n    \"GRAD_ACCUM_STEPS\": 4,\n\n    # Generation defaults\n    \"GEN_BEAMS\": 5,\n    \"GEN_MAX_NEW_TOKENS\": 128,\n}\n\nCFG","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:37:21.501182Z","iopub.execute_input":"2026-02-19T23:37:21.501878Z","iopub.status.idle":"2026-02-19T23:37:21.512217Z","shell.execute_reply.started":"2026-02-19T23:37:21.501842Z","shell.execute_reply":"2026-02-19T23:37:21.511418Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'LIBRISPEECH_ROOT': '/kaggle/input/datasets/pypiahmad/librispeech-asr-corpus',\n 'CV_ROOT': '/kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice',\n 'CV_LANG_DIR': 'commonvoice-v24_en-AU',\n 'CV_AUDIO_DIR': 'audio_files',\n 'CV_MAIN_CSV': 'commonvoice-v24_en-AU.csv',\n 'CV_SPLIT_CSV': 'commonvoice-v24_en-AU-split.csv',\n 'MODEL_NAME': 'openai/whisper-small.en',\n 'MAX_LS_TRAIN_ROWS': 60000,\n 'MAX_LS_DEV_ROWS': 5000,\n 'MAX_CV_TRAIN_ROWS': 25000,\n 'MAX_CV_DEV_ROWS': 3000,\n 'MAX_CV_TEST_ROWS': 3000,\n 'MIN_AUDIO_SEC': 1.0,\n 'MAX_AUDIO_SEC': 20.0,\n 'MAX_LABEL_CHARS': 220,\n 'USE_QUALITY_FILTERING': True,\n 'USE_CURRICULUM': True,\n 'USE_SPEAKER_BALANCED_SAMPLING': True,\n 'USE_SPECAUG': True,\n 'USE_LORA': True,\n 'USE_8BIT_OPTIM': True,\n 'USE_TRANSCRIPT_DENOISE': True,\n 'USE_DECODE_TUNING': True,\n 'QUALITY_METRIC_MAX_SAMPLES': 12000,\n 'DENOISE_SCORE_MAX_SAMPLES': 8000,\n 'DENOISE_DROP_FRACTION': 0.12,\n 'STAGE_A_MAX_STEPS': 1200,\n 'STAGE_A_LR': 0.0001,\n 'STAGE_B_MAX_STEPS': 900,\n 'STAGE_B_LR': 7e-05,\n 'PER_DEVICE_TRAIN_BS': 2,\n 'PER_DEVICE_EVAL_BS': 2,\n 'GRAD_ACCUM_STEPS': 4,\n 'GEN_BEAMS': 5,\n 'GEN_MAX_NEW_TOKENS': 128}"},"metadata":{}}],"execution_count":4},{"id":"31f0d62f-6ab1-48fd-a399-eb13fd6e8fcd","cell_type":"markdown","source":"## 2) Auto-detect dataset roots based on your folder structure","metadata":{}},{"id":"43e8c835-0972-421a-865c-bc4d9b08d594","cell_type":"code","source":"# Cell 4 — Resolve dataset roots + auto-find Common Voice CSVs robustly\n\nfrom pathlib import Path\nimport os\n\nLIBRISPEECH_ROOT = Path(CFG[\"LIBRISPEECH_ROOT\"])\nCV_ROOT = Path(CFG[\"CV_ROOT\"])\n\nprint(\"LIBRISPEECH_ROOT:\", LIBRISPEECH_ROOT)\nprint(\"CV_ROOT (given):\", CV_ROOT)\n\n# ---- LibriSpeech checks ----\nassert (LIBRISPEECH_ROOT / \"train-clean-360\").exists(), \"Missing train-clean-360\"\nassert (LIBRISPEECH_ROOT / \"dev-clean\").exists(), \"Missing dev-clean\"\nassert (LIBRISPEECH_ROOT / \"test-clean\").exists(), \"Missing test-clean\"\nprint(\"✅ LibriSpeech structure OK\")\n\n# ---- Common Voice: sometimes there is an extra nested directory ----\ncandidate_cv_roots = [\n    CV_ROOT,\n    CV_ROOT / \"mozilla-commonvoice\",\n    CV_ROOT / \"mozilla_commonvoice\",\n    CV_ROOT / \"commonvoice\",\n]\n\nREAL_CV_ROOT = None\nfor r in candidate_cv_roots:\n    if (r / CFG[\"CV_LANG_DIR\"]).exists():\n        REAL_CV_ROOT = r\n        break\n\nassert REAL_CV_ROOT is not None, (\n    f\"Could not find {CFG['CV_LANG_DIR']} under any of these roots:\\n\" +\n    \"\\n\".join([str(x) for x in candidate_cv_roots])\n)\n\nprint(\"✅ REAL_CV_ROOT:\", REAL_CV_ROOT)\n\nCV_LANG_PATH = REAL_CV_ROOT / CFG[\"CV_LANG_DIR\"]\nCV_AUDIO_ROOT = CV_LANG_PATH / CFG[\"CV_AUDIO_DIR\"]\nprint(\"CV_LANG_PATH:\", CV_LANG_PATH)\nprint(\"CV_AUDIO_ROOT:\", CV_AUDIO_ROOT)\nassert CV_AUDIO_ROOT.exists(), f\"Missing audio folder: {CV_AUDIO_ROOT}\"\n\n# ---- Find CSVs (exact name if possible, else search) ----\ndef find_csv(preferred_name: str, must_contain: list[str], search_roots: list[Path]) -> Path:\n    # 1) try exact\n    for root in search_roots:\n        p = root / preferred_name\n        if p.exists():\n            return p\n\n    # 2) search all csv files under roots\n    matches = []\n    for root in search_roots:\n        for p in root.rglob(\"*.csv\"):\n            name = p.name.lower()\n            ok = all(s.lower() in name for s in must_contain)\n            if ok:\n                matches.append(p)\n\n    if not matches:\n        # Print helpful debug listing\n        print(\"\\n--- CSV files found under REAL_CV_ROOT (top 50) ---\")\n        all_csv = list(REAL_CV_ROOT.rglob(\"*.csv\"))[:50]\n        for p in all_csv:\n            print(p)\n        raise FileNotFoundError(\n            f\"Could not locate a CSV for '{preferred_name}' with must_contain={must_contain}\"\n        )\n\n    # Prefer shortest path (usually closest to root), stable sort\n    matches = sorted(matches, key=lambda x: (len(str(x)), str(x)))\n    return matches[0]\n\nsearch_roots = [REAL_CV_ROOT, CV_LANG_PATH]\n\n# Main CSV (non-split): must include en-au and not necessarily \"split\"\nmain_csv_path = find_csv(\n    CFG[\"CV_MAIN_CSV\"],\n    must_contain=[\"en-au\"],     # adjust if needed\n    search_roots=search_roots\n)\n\n# Split CSV: must include split + en-au\nsplit_csv_path = find_csv(\n    CFG[\"CV_SPLIT_CSV\"],\n    must_contain=[\"split\", \"en-au\"],\n    search_roots=search_roots\n)\n\nCFG[\"REAL_CV_ROOT\"] = str(REAL_CV_ROOT)\nCFG[\"CV_MAIN_CSV_PATH\"] = str(main_csv_path)\nCFG[\"CV_SPLIT_CSV_PATH\"] = str(split_csv_path)\nCFG[\"CV_AUDIO_ROOT\"] = str(CV_AUDIO_ROOT)\n\nprint(\"✅ CV_MAIN_CSV_PATH:\", CFG[\"CV_MAIN_CSV_PATH\"])\nprint(\"✅ CV_SPLIT_CSV_PATH:\", CFG[\"CV_SPLIT_CSV_PATH\"])\nprint(\"✅ CV_AUDIO_ROOT:\", CFG[\"CV_AUDIO_ROOT\"])","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:37:26.387905Z","iopub.execute_input":"2026-02-19T23:37:26.388743Z","iopub.status.idle":"2026-02-19T23:37:26.440529Z","shell.execute_reply.started":"2026-02-19T23:37:26.388713Z","shell.execute_reply":"2026-02-19T23:37:26.439660Z"},"trusted":true},"outputs":[{"name":"stdout","text":"LIBRISPEECH_ROOT: /kaggle/input/datasets/pypiahmad/librispeech-asr-corpus\nCV_ROOT (given): /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice\n✅ LibriSpeech structure OK\n✅ REAL_CV_ROOT: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice\nCV_LANG_PATH: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU\nCV_AUDIO_ROOT: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/audio_files\n✅ CV_MAIN_CSV_PATH: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU.csv\n✅ CV_SPLIT_CSV_PATH: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU-split.csv\n✅ CV_AUDIO_ROOT: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/audio_files\n","output_type":"stream"}],"execution_count":5},{"id":"2c7bd4dc-6c79-4efb-9b20-240795c31763","cell_type":"markdown","source":"## 3) LibriSpeech parsing → dataframe (speaker_id, text, audio_path, split)","metadata":{}},{"id":"8fe13c22-b073-4d3a-97da-1ec9b9528917","cell_type":"code","source":"# Cell 5 — Parse LibriSpeech transcripts (.trans.txt) + audio paths\n\ndef parse_librispeech_split(split_dir: Path, split_name: str) -> pd.DataFrame:\n    rows = []\n    trans_files = list(split_dir.rglob(\"*.trans.txt\"))\n    for tf in trans_files:\n        try:\n            with open(tf, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    parts = line.split(\" \", 1)\n                    if len(parts) != 2:\n                        continue\n                    utt_id, text = parts\n                    audio_path = tf.parent / f\"{utt_id}.flac\"\n                    if not audio_path.exists():\n                        continue\n                    speaker_id = tf.parent.parent.name\n                    chapter_id = tf.parent.name\n                    rows.append({\n                        \"dataset\": \"librispeech\",\n                        \"split\": split_name,\n                        \"speaker_id\": str(speaker_id),\n                        \"chapter_id\": str(chapter_id),\n                        \"utt_id\": utt_id,\n                        \"text\": text,\n                        \"audio_path\": str(audio_path),\n                    })\n        except Exception as e:\n            print(\"Failed reading\", tf, e)\n    return pd.DataFrame(rows)\n\nls_train = parse_librispeech_split(Path(LIBRISPEECH_ROOT) / \"train-clean-360\", \"train\")\nls_dev   = parse_librispeech_split(Path(LIBRISPEECH_ROOT) / \"dev-clean\", \"dev\")\nls_test  = parse_librispeech_split(Path(LIBRISPEECH_ROOT) / \"test-clean\", \"test\")\n\nprint(ls_train.shape, ls_dev.shape, ls_test.shape)\n\n# Caps for speed\nif CFG[\"MAX_LS_TRAIN_ROWS\"]:\n    ls_train = ls_train.sample(n=min(CFG[\"MAX_LS_TRAIN_ROWS\"], len(ls_train)), random_state=SEED)\nif CFG[\"MAX_LS_DEV_ROWS\"]:\n    ls_dev = ls_dev.sample(n=min(CFG[\"MAX_LS_DEV_ROWS\"], len(ls_dev)), random_state=SEED)\n\nls_df = pd.concat([ls_train, ls_dev, ls_test], ignore_index=True)\nls_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:37:31.627337Z","iopub.execute_input":"2026-02-19T23:37:31.627653Z","iopub.status.idle":"2026-02-19T23:43:35.798970Z","shell.execute_reply.started":"2026-02-19T23:37:31.627626Z","shell.execute_reply":"2026-02-19T23:43:35.798208Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(104014, 7) (2703, 7) (2620, 7)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       dataset  split speaker_id chapter_id            utt_id  \\\n0  librispeech  train       6160      44912   6160-44912-0071   \n1  librispeech  train       7276     284424  7276-284424-0036   \n2  librispeech  train        359     133630   359-133630-0007   \n3  librispeech  train       5935      43322   5935-43322-0009   \n4  librispeech  train       3790     140725  3790-140725-0031   \n\n                                                text  \\\n0  WHO COULD SPEAK FRENCH AND WHO HAD LEARNED GER...   \n1  BUTTON BRIGHT SHOOK HIS HEAD A BOAT CAN'T LAND...   \n2  AND WANTED TO GAIN THE EXPERIENCE AND NOW THE ...   \n3  HERE IS THE ORDER OF WORSHIP FOR THE FEAST OF ...   \n4  BUT ONE OF THE HELPERS GOT THE KEYS FROM MISSU...   \n\n                                          audio_path  \n0  /kaggle/input/datasets/pypiahmad/librispeech-a...  \n1  /kaggle/input/datasets/pypiahmad/librispeech-a...  \n2  /kaggle/input/datasets/pypiahmad/librispeech-a...  \n3  /kaggle/input/datasets/pypiahmad/librispeech-a...  \n4  /kaggle/input/datasets/pypiahmad/librispeech-a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>split</th>\n      <th>speaker_id</th>\n      <th>chapter_id</th>\n      <th>utt_id</th>\n      <th>text</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>librispeech</td>\n      <td>train</td>\n      <td>6160</td>\n      <td>44912</td>\n      <td>6160-44912-0071</td>\n      <td>WHO COULD SPEAK FRENCH AND WHO HAD LEARNED GER...</td>\n      <td>/kaggle/input/datasets/pypiahmad/librispeech-a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>librispeech</td>\n      <td>train</td>\n      <td>7276</td>\n      <td>284424</td>\n      <td>7276-284424-0036</td>\n      <td>BUTTON BRIGHT SHOOK HIS HEAD A BOAT CAN'T LAND...</td>\n      <td>/kaggle/input/datasets/pypiahmad/librispeech-a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>librispeech</td>\n      <td>train</td>\n      <td>359</td>\n      <td>133630</td>\n      <td>359-133630-0007</td>\n      <td>AND WANTED TO GAIN THE EXPERIENCE AND NOW THE ...</td>\n      <td>/kaggle/input/datasets/pypiahmad/librispeech-a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>librispeech</td>\n      <td>train</td>\n      <td>5935</td>\n      <td>43322</td>\n      <td>5935-43322-0009</td>\n      <td>HERE IS THE ORDER OF WORSHIP FOR THE FEAST OF ...</td>\n      <td>/kaggle/input/datasets/pypiahmad/librispeech-a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>librispeech</td>\n      <td>train</td>\n      <td>3790</td>\n      <td>140725</td>\n      <td>3790-140725-0031</td>\n      <td>BUT ONE OF THE HELPERS GOT THE KEYS FROM MISSU...</td>\n      <td>/kaggle/input/datasets/pypiahmad/librispeech-a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"id":"40897413-8d4f-49ba-bd9b-34139bde637c","cell_type":"markdown","source":"## 4) Common Voice parsing → dataframe (uses split CSV + audio_files)","metadata":{}},{"id":"9c5ac731-657f-4eef-8003-61f7a769acee","cell_type":"code","source":"# Cell 6 — Load Common Voice CSVs (robust) + build paths + ensure speaker_id + create split\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# Resolve paths (uses CFG[\"CV_MAIN_CSV_PATH\"] etc. if already set)\ndef resolve_cv_paths(CFG):\n    CV_ROOT = Path(CFG[\"CV_ROOT\"])\n    lang_dir = CFG[\"CV_LANG_DIR\"]\n\n    candidates = [CV_ROOT, CV_ROOT / \"mozilla-commonvoice\", CV_ROOT / \"mozilla_commonvoice\", CV_ROOT / \"commonvoice\", CV_ROOT / \"data\"]\n    real_root = None\n    for r in candidates:\n        if (r / lang_dir).exists():\n            real_root = r\n            break\n    if real_root is None:\n        raise FileNotFoundError(f\"Could not find '{lang_dir}' under: {candidates}\")\n\n    lang_path = real_root / lang_dir\n    audio_root = lang_path / CFG[\"CV_AUDIO_DIR\"]\n    if not audio_root.exists():\n        raise FileNotFoundError(f\"Missing audio folder: {audio_root}\")\n\n    main_csv = CFG.get(\"CV_MAIN_CSV_PATH\")\n    split_csv = CFG.get(\"CV_SPLIT_CSV_PATH\")\n    if main_csv is None or split_csv is None:\n        # If not already resolved, fall back to exact names under lang folder\n        main_csv = str(lang_path / CFG[\"CV_MAIN_CSV\"])\n        split_csv = str(lang_path / CFG[\"CV_SPLIT_CSV\"])\n        if not Path(main_csv).exists() or not Path(split_csv).exists():\n            raise FileNotFoundError(\"CSV paths not found; please ensure CFG paths are correct.\")\n\n    CFG[\"REAL_CV_ROOT\"] = str(real_root)\n    CFG[\"CV_AUDIO_ROOT\"] = str(audio_root)\n    CFG[\"CV_MAIN_CSV_PATH\"] = str(main_csv)\n    CFG[\"CV_SPLIT_CSV_PATH\"] = str(split_csv)\n\n    return Path(main_csv), Path(split_csv), Path(audio_root)\n\ncv_main_path, cv_split_path, cv_audio_root = resolve_cv_paths(CFG)\nprint(\"CV_MAIN_CSV_PATH:\", cv_main_path)\nprint(\"CV_SPLIT_CSV_PATH:\", cv_split_path)\nprint(\"CV_AUDIO_ROOT:\", cv_audio_root)\n\ncv_main = pd.read_csv(cv_main_path)\ncv_split = pd.read_csv(cv_split_path)\n\n# Merge keys\nmerge_keys = [k for k in [\"path\", \"client_id\", \"sentence\"] if k in cv_main.columns and k in cv_split.columns]\nif not merge_keys:\n    if \"path\" in cv_split.columns and \"path\" in cv_main.columns:\n        merge_keys = [\"path\"]\n    else:\n        overlap = [c for c in cv_split.columns if c in cv_main.columns]\n        if not overlap:\n            raise ValueError(\"No overlapping columns found to merge CV main and split CSVs.\")\n        merge_keys = overlap[:1]\n        print(\"⚠️ Using fallback merge key:\", merge_keys)\n\ncv = cv_split.merge(cv_main, on=merge_keys, how=\"left\", suffixes=(\"_split\", \"\"))\ncv[\"dataset\"] = \"commonvoice\"\n\n# Audio filename column\nif \"path\" in cv.columns:\n    rel_audio = cv[\"path\"].astype(str)\nelif \"filename\" in cv.columns:\n    rel_audio = cv[\"filename\"].astype(str)\nelse:\n    alt = None\n    for cand in [\"clip\", \"file\", \"audio\", \"wav_filename\"]:\n        if cand in cv.columns:\n            alt = cand\n            break\n    if alt is None:\n        raise ValueError(\"No audio filename column found (expected 'path' or 'filename').\")\n    rel_audio = cv[alt].astype(str)\n\ncv[\"audio_path\"] = rel_audio.apply(lambda p: str(Path(cv_audio_root) / p))\n\n# Transcript column\ntext_col = None\nfor cand in [\"sentence\", \"text\", \"transcript\"]:\n    if cand in cv.columns:\n        text_col = cand\n        break\nassert text_col is not None, \"No transcript column found (expected 'sentence' or 'text' or 'transcript')\"\ncv[\"text\"] = cv[text_col].astype(str)\n\n# ✅ IMPORTANT: create speaker_id BEFORE any split fallback\nif \"client_id\" in cv.columns:\n    cv[\"speaker_id\"] = cv[\"client_id\"].astype(str)\nelif \"speaker_id\" in cv.columns:\n    cv[\"speaker_id\"] = cv[\"speaker_id\"].astype(str)\nelse:\n    # fallback speaker_id using path prefix (not perfect but prevents crash)\n    # Common Voice paths often like: <something>.mp3, so this becomes \"unknown\"\n    cv[\"speaker_id\"] = \"unknown\"\n\n# Split handling\nsplit_col = None\nfor cand in [\"split\", \"set\", \"subset\", \"partition\", \"data_split\", \"group\", \"fold\"]:\n    if cand in cv.columns:\n        split_col = cand\n        break\n\nif split_col is not None:\n    cv[\"split\"] = cv[split_col].astype(str).str.lower()\nelse:\n    cols_lower = {c.lower(): c for c in cv.columns}\n\n    def has_cols(*names):\n        return all(n in cols_lower for n in names)\n\n    if has_cols(\"train\", \"dev\", \"test\"):\n        c_train, c_dev, c_test = cols_lower[\"train\"], cols_lower[\"dev\"], cols_lower[\"test\"]\n        idx = cv[[c_train, c_dev, c_test]].astype(float).values.argmax(axis=1)\n        cv[\"split\"] = np.where(idx == 0, \"train\", np.where(idx == 1, \"dev\", \"test\"))\n\n    elif has_cols(\"train\", \"validation\", \"test\"):\n        c_train, c_val, c_test = cols_lower[\"train\"], cols_lower[\"validation\"], cols_lower[\"test\"]\n        idx = cv[[c_train, c_val, c_test]].astype(float).values.argmax(axis=1)\n        cv[\"split\"] = np.where(idx == 0, \"train\", np.where(idx == 1, \"dev\", \"test\"))\n\n    elif has_cols(\"train\", \"valid\", \"test\"):\n        c_train, c_val, c_test = cols_lower[\"train\"], cols_lower[\"valid\"], cols_lower[\"test\"]\n        idx = cv[[c_train, c_val, c_test]].astype(float).values.argmax(axis=1)\n        cv[\"split\"] = np.where(idx == 0, \"train\", np.where(idx == 1, \"dev\", \"test\"))\n\n    else:\n        # Speaker-safe split (90/5/5) — works now because speaker_id exists\n        print(\"⚠️ No split info found in CSVs. Creating speaker-safe split (90/5/5).\")\n\n        speakers = pd.Series(cv[\"speaker_id\"].astype(str).unique())\n        speakers = speakers.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n\n        n = len(speakers)\n        n_train = int(0.90 * n)\n        n_dev = int(0.05 * n)\n\n        train_spk = set(speakers.iloc[:n_train])\n        dev_spk = set(speakers.iloc[n_train:n_train + n_dev])\n\n        def assign_split(spk):\n            if spk in train_spk: return \"train\"\n            if spk in dev_spk:   return \"dev\"\n            return \"test\"\n\n        cv[\"split\"] = cv[\"speaker_id\"].astype(str).map(assign_split)\n\ncv[\"split\"] = cv[\"split\"].replace({\"val\": \"dev\", \"valid\": \"dev\", \"validation\": \"dev\"}).astype(str).str.lower()\nprint(\"Common Voice split counts:\", cv[\"split\"].value_counts().to_dict())\n\n# Keep cols\nkeep_cols = [\"dataset\", \"split\", \"speaker_id\", \"text\", \"audio_path\"]\nfor dcol in [\"gender\", \"age\", \"accent\", \"locale\"]:\n    if dcol in cv.columns:\n        keep_cols.append(dcol)\n\ncv_df = cv[keep_cols].copy()\n\n# Cap splits\ndef cap_split(df, split, n):\n    if n is None:\n        return df\n    sub = df[df[\"split\"] == split]\n    if len(sub) <= n:\n        return df\n    sub = sub.sample(n=n, random_state=SEED)\n    return pd.concat([df[df[\"split\"] != split], sub], ignore_index=True)\n\ncv_df = cap_split(cv_df, \"train\", CFG[\"MAX_CV_TRAIN_ROWS\"])\ncv_df = cap_split(cv_df, \"dev\",   CFG[\"MAX_CV_DEV_ROWS\"])\ncv_df = cap_split(cv_df, \"test\",  CFG[\"MAX_CV_TEST_ROWS\"])\n\ncv_df.head(), cv_df[\"split\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:43:55.331258Z","iopub.execute_input":"2026-02-19T23:43:55.331627Z","iopub.status.idle":"2026-02-19T23:43:57.422201Z","shell.execute_reply.started":"2026-02-19T23:43:55.331596Z","shell.execute_reply":"2026-02-19T23:43:57.421383Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CV_MAIN_CSV_PATH: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU.csv\nCV_SPLIT_CSV_PATH: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU-split.csv\nCV_AUDIO_ROOT: /kaggle/input/datasets/eddiehoogewerf/mozilla-commonvoice/commonvoice-v24_en-AU/audio_files\n⚠️ No split info found in CSVs. Creating speaker-safe split (90/5/5).\nCommon Voice split counts: {'train': 52188, 'dev': 2336, 'test': 1149}\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(       dataset split                                         speaker_id  \\\n 0  commonvoice   dev  18bea6bb076cd9638518d93b4af353c3c329d059789e11...   \n 1  commonvoice   dev  46979dc7ff629110c30910e33e07360e8a6b1164d886b8...   \n 2  commonvoice  test  5164c1f810f1164f347010cf76b6193eda6fcc8ccbe746...   \n 3  commonvoice  test  571c11f069b81ffb3e5548a57b0c215c12f4c1d8d64baa...   \n 4  commonvoice   dev  85ddb3498519d2676393ceab1451c21bc87b5ff4e3b23a...   \n \n                                                 text  \\\n 0     He has also served in the Chamber of Deputies.   \n 1  Severe environmental issues include deforestat...   \n 2    Sean has deteriorated to the point of dementia.   \n 3  Let us face the door, and welcome in our proph...   \n 4  Elephant bells had a marked flare below the kn...   \n \n                                           audio_path gender  age locale  \n 0  /kaggle/input/datasets/eddiehoogewerf/mozilla-...    NaN  NaN     en  \n 1  /kaggle/input/datasets/eddiehoogewerf/mozilla-...    NaN  NaN     en  \n 2  /kaggle/input/datasets/eddiehoogewerf/mozilla-...    NaN  NaN     en  \n 3  /kaggle/input/datasets/eddiehoogewerf/mozilla-...    NaN  NaN     en  \n 4  /kaggle/input/datasets/eddiehoogewerf/mozilla-...    NaN  NaN     en  ,\n split\n train    25000\n dev       2336\n test      1149\n Name: count, dtype: int64)"},"metadata":{}}],"execution_count":7},{"id":"d88b9516-e5ce-4475-b43d-7644f281d479","cell_type":"markdown","source":"## 5) Standard text normalization (training + evaluation)","metadata":{}},{"id":"d222fbfb-cb0e-462b-b0a5-ce4681ded45f","cell_type":"code","source":"# Cell 7 — Normalization utilities (helps CER)\n\n_basic_punct = re.compile(r\"[\\.,!?;:\\\"\\(\\)\\[\\]\\{\\}<>\\\\/\\|@#\\$%\\^&\\*_=\\+~`]+\")\n\ndef normalize_text(s: str) -> str:\n    s = str(s).strip().lower()\n    s = s.replace(\"’\", \"'\")\n    s = _basic_punct.sub(\" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\nls_df[\"text_norm\"] = ls_df[\"text\"].map(normalize_text)\ncv_df[\"text_norm\"] = cv_df[\"text\"].map(normalize_text)\n\nls_df[[\"text\",\"text_norm\"]].head()\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:44:12.524160Z","iopub.execute_input":"2026-02-19T23:44:12.524905Z","iopub.status.idle":"2026-02-19T23:44:13.863066Z","shell.execute_reply.started":"2026-02-19T23:44:12.524864Z","shell.execute_reply":"2026-02-19T23:44:13.862209Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  WHO COULD SPEAK FRENCH AND WHO HAD LEARNED GER...   \n1  BUTTON BRIGHT SHOOK HIS HEAD A BOAT CAN'T LAND...   \n2  AND WANTED TO GAIN THE EXPERIENCE AND NOW THE ...   \n3  HERE IS THE ORDER OF WORSHIP FOR THE FEAST OF ...   \n4  BUT ONE OF THE HELPERS GOT THE KEYS FROM MISSU...   \n\n                                           text_norm  \n0  who could speak french and who had learned ger...  \n1  button bright shook his head a boat can't land...  \n2  and wanted to gain the experience and now the ...  \n3  here is the order of worship for the feast of ...  \n4  but one of the helpers got the keys from missu...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHO COULD SPEAK FRENCH AND WHO HAD LEARNED GER...</td>\n      <td>who could speak french and who had learned ger...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BUTTON BRIGHT SHOOK HIS HEAD A BOAT CAN'T LAND...</td>\n      <td>button bright shook his head a boat can't land...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AND WANTED TO GAIN THE EXPERIENCE AND NOW THE ...</td>\n      <td>and wanted to gain the experience and now the ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HERE IS THE ORDER OF WORSHIP FOR THE FEAST OF ...</td>\n      <td>here is the order of worship for the feast of ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BUT ONE OF THE HELPERS GOT THE KEYS FROM MISSU...</td>\n      <td>but one of the helpers got the keys from missu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"id":"99c5b0ec-53ba-42b8-9bf8-6da0e055c6ef","cell_type":"markdown","source":"## 6) Duration + quality metrics (silence ratio, clipping, RMS loudness) for filtering/curriculum","metadata":{}},{"id":"7fffeb99-ba19-4a82-9b56-3ef8c7e7f5a7","cell_type":"code","source":"# Cell 8 — Compute quality metrics on a sample for speed (FIXED: always creates metric columns)\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torchaudio\n\nQUALITY_COLS = [\"duration_sec\", \"silence_ratio\", \"clipping_rate\", \"rms_mean_db\"]\n\ndef load_audio_16k(path: str):\n    \"\"\"Try torchaudio first; fallback to librosa if torchaudio can't decode (e.g., mp3).\"\"\"\n    try:\n        wav, sr = torchaudio.load(path)\n        wav = wav.mean(dim=0).numpy()\n    except Exception:\n        wav, sr = librosa.load(path, sr=None, mono=True)\n\n    if sr != 16000:\n        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n        sr = 16000\n    return wav, sr\n\ndef compute_quality_metrics(path: str, frame_ms=20, hop_ms=10, silence_db=-40.0):\n    try:\n        y, sr = load_audio_16k(path)\n        if y is None or len(y) < 160:\n            return None\n\n        dur = len(y) / sr\n        frame = max(int(sr * frame_ms / 1000), 16)\n        hop = max(int(sr * hop_ms / 1000), 8)\n\n        rms = librosa.feature.rms(y=y, frame_length=frame, hop_length=hop)[0]\n        rms_db = 20*np.log10(np.maximum(rms, 1e-10))\n        silence_ratio = float(np.mean(rms_db < silence_db))\n\n        clipping_rate = float(np.mean(np.abs(y) >= 0.999))\n\n        rms_mean = float(np.mean(rms))\n        rms_mean_db = float(20*np.log10(max(rms_mean, 1e-10)))\n\n        return {\n            \"duration_sec\": float(dur),\n            \"silence_ratio\": silence_ratio,\n            \"clipping_rate\": clipping_rate,\n            \"rms_mean_db\": rms_mean_db\n        }\n    except Exception:\n        return None\n\ndef add_quality_columns(df: pd.DataFrame, max_samples: int, seed: int = 42) -> pd.DataFrame:\n    \"\"\"\n    Returns a copy of df with QUALITY_COLS filled for a sampled subset.\n    IMPORTANT: Always creates QUALITY_COLS even if nothing was computed.\n    \"\"\"\n    df = df.copy()\n\n    # ✅ Ensure these columns exist no matter what\n    for c in QUALITY_COLS:\n        if c not in df.columns:\n            df[c] = np.nan\n\n    if len(df) == 0:\n        return df\n\n    sample_idx = df.sample(n=min(max_samples, len(df)), random_state=seed).index\n\n    metrics = {}\n    for i, idx in enumerate(sample_idx):\n        m = compute_quality_metrics(df.loc[idx, \"audio_path\"])\n        if m is not None:\n            metrics[idx] = m\n        if (i + 1) % 500 == 0:\n            print(f\"  processed {i+1}/{len(sample_idx)}\")\n\n    # ✅ Build mdf with guaranteed columns\n    mdf = pd.DataFrame.from_dict(metrics, orient=\"index\")\n    for c in QUALITY_COLS:\n        if c not in mdf.columns:\n            mdf[c] = np.nan\n\n    # assign back\n    for c in QUALITY_COLS:\n        if len(mdf) > 0:\n            df.loc[mdf.index, c] = mdf[c].values\n\n    return df\n\n# Recompute\nls_q = add_quality_columns(ls_df[ls_df[\"split\"].isin([\"train\",\"dev\"])], CFG[\"QUALITY_METRIC_MAX_SAMPLES\"], SEED)\ncv_q = add_quality_columns(cv_df[cv_df[\"split\"].isin([\"train\",\"dev\"])], CFG[\"QUALITY_METRIC_MAX_SAMPLES\"], SEED)\n\nprint(\"ls_q cols:\", ls_q.columns.tolist())\nprint(\"cv_q cols:\", cv_q.columns.tolist())\n\n# ✅ Your original merge lines now work safely\nls_df = ls_df.merge(ls_q[[\"audio_path\"] + QUALITY_COLS], on=\"audio_path\", how=\"left\")\ncv_df = cv_df.merge(cv_q[[\"audio_path\"] + QUALITY_COLS], on=\"audio_path\", how=\"left\")\n\nls_df[[\"split\"] + QUALITY_COLS].head()","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:44:18.219932Z","iopub.execute_input":"2026-02-19T23:44:18.220273Z","iopub.status.idle":"2026-02-19T23:51:42.411196Z","shell.execute_reply.started":"2026-02-19T23:44:18.220246Z","shell.execute_reply":"2026-02-19T23:51:42.410223Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"name":"stdout","text":"  processed 500/12000\n  processed 1000/12000\n  processed 1500/12000\n  processed 2000/12000\n  processed 2500/12000\n  processed 3000/12000\n  processed 3500/12000\n  processed 4000/12000\n  processed 4500/12000\n  processed 5000/12000\n  processed 5500/12000\n  processed 6000/12000\n  processed 6500/12000\n  processed 7000/12000\n  processed 7500/12000\n  processed 8000/12000\n  processed 8500/12000\n  processed 9000/12000\n  processed 9500/12000\n  processed 10000/12000\n  processed 10500/12000\n  processed 11000/12000\n  processed 11500/12000\n  processed 12000/12000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"name":"stdout","text":"  processed 500/12000\n  processed 1000/12000\n  processed 1500/12000\n  processed 2000/12000\n  processed 2500/12000\n  processed 3000/12000\n  processed 3500/12000\n  processed 4000/12000\n  processed 4500/12000\n  processed 5000/12000\n  processed 5500/12000\n  processed 6000/12000\n  processed 6500/12000\n  processed 7000/12000\n  processed 7500/12000\n  processed 8000/12000\n  processed 8500/12000\n  processed 9000/12000\n  processed 9500/12000\n  processed 10000/12000\n  processed 10500/12000\n  processed 11000/12000\n  processed 11500/12000\n  processed 12000/12000\nls_q cols: ['dataset', 'split', 'speaker_id', 'chapter_id', 'utt_id', 'text', 'audio_path', 'text_norm', 'duration_sec', 'silence_ratio', 'clipping_rate', 'rms_mean_db']\ncv_q cols: ['dataset', 'split', 'speaker_id', 'text', 'audio_path', 'gender', 'age', 'locale', 'text_norm', 'duration_sec', 'silence_ratio', 'clipping_rate', 'rms_mean_db']\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   split  duration_sec  silence_ratio  clipping_rate  rms_mean_db\n0  train           NaN            NaN            NaN          NaN\n1  train           NaN            NaN            NaN          NaN\n2  train           NaN            NaN            NaN          NaN\n3  train           NaN            NaN            NaN          NaN\n4  train         11.52       0.208153            0.0   -27.794453","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>split</th>\n      <th>duration_sec</th>\n      <th>silence_ratio</th>\n      <th>clipping_rate</th>\n      <th>rms_mean_db</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train</td>\n      <td>11.52</td>\n      <td>0.208153</td>\n      <td>0.0</td>\n      <td>-27.794453</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"id":"e0ec647b-b3f9-4e57-b589-6bdc7f5d4e49","cell_type":"markdown","source":"## 7) Quality-aware filtering + curriculum bins","metadata":{}},{"id":"e89fee19-bfc0-4344-8552-0b084e76b030","cell_type":"code","source":"# Cell 9 — Core filters + optional quality filtering + curriculum bins\n\ndef apply_core_filters(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    if \"duration_sec\" in df.columns:\n        df = df[(df[\"duration_sec\"].isna()) | ((df[\"duration_sec\"] >= CFG[\"MIN_AUDIO_SEC\"]) & (df[\"duration_sec\"] <= CFG[\"MAX_AUDIO_SEC\"]))]\n    df = df[df[\"text_norm\"].str.len() > 0]\n    df = df[df[\"text_norm\"].str.len() <= CFG[\"MAX_LABEL_CHARS\"]]\n    return df.reset_index(drop=True)\n\ndef quality_score_row(r):\n    sil = r.get(\"silence_ratio\", np.nan)\n    clip = r.get(\"clipping_rate\", np.nan)\n    loud = r.get(\"rms_mean_db\", np.nan)\n\n    sil_pen = 0.0 if np.isnan(sil) else -2.0 * sil\n    clip_pen = 0.0 if np.isnan(clip) else -8.0 * clip\n    loud_pen = 0.0\n    if not np.isnan(loud):\n        loud_pen = -abs(np.clip(loud, -60, 0) - (-22.0)) / 18.0\n    return float(sil_pen + clip_pen + loud_pen)\n\ndef apply_quality_filter_and_curriculum(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"quality_score\"] = df.apply(quality_score_row, axis=1)\n\n    if CFG[\"USE_QUALITY_FILTERING\"]:\n        q = df[\"quality_score\"].dropna()\n        if len(q) > 500:\n            low_cut = q.quantile(0.05)\n            df = df[(df[\"quality_score\"].isna()) | (df[\"quality_score\"] >= low_cut)].reset_index(drop=True)\n\n    if CFG[\"USE_CURRICULUM\"]:\n        q = df[\"quality_score\"].dropna()\n        if len(q) > 500:\n            q1, q2 = q.quantile(0.33), q.quantile(0.66)\n            def bin_fn(x):\n                if np.isnan(x): return 1\n                if x < q1: return 2\n                if x < q2: return 1\n                return 0\n            df[\"curriculum_bin\"] = df[\"quality_score\"].map(bin_fn).astype(int)\n        else:\n            df[\"curriculum_bin\"] = 1\n    else:\n        df[\"curriculum_bin\"] = 0\n\n    return df\n\nls_df_f = apply_quality_filter_and_curriculum(apply_core_filters(ls_df))\ncv_df_f = apply_quality_filter_and_curriculum(apply_core_filters(cv_df))\n\nprint(\"LibriSpeech after filters:\", ls_df_f.shape, ls_df_f[\"split\"].value_counts().to_dict())\nprint(\"Common Voice after filters:\", cv_df_f.shape, cv_df_f[\"split\"].value_counts().to_dict())\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:51:45.673911Z","iopub.execute_input":"2026-02-19T23:51:45.674882Z","iopub.status.idle":"2026-02-19T23:51:47.025611Z","shell.execute_reply.started":"2026-02-19T23:51:45.674848Z","shell.execute_reply":"2026-02-19T23:51:47.024883Z"},"trusted":true},"outputs":[{"name":"stdout","text":"LibriSpeech after filters: (45049, 14) {'train': 40356, 'test': 2382, 'dev': 2311}\nCommon Voice after filters: (27058, 15) {'train': 23658, 'dev': 2251, 'test': 1149}\n","output_type":"stream"}],"execution_count":10},{"id":"8eb9ee58-f425-46ac-b535-cea50bb5d155","cell_type":"markdown","source":"## 8) Build Hugging Face datasets + audio casting","metadata":{}},{"id":"6e72113f-1347-4af2-bc81-b003e59cc119","cell_type":"code","source":"# Cell 10 — Convert dataframes to HF datasets (keep audio_path as STRING; decode in collator)\n# Why: Common Voice clips are often .mp3; datasets.Audio decoding can be fragile on Kaggle.\n# We'll decode with torchaudio/librosa inside the collator.\n\nfrom datasets import Dataset, DatasetDict\n\ndef to_hf_dataset(df: pd.DataFrame) -> DatasetDict:\n    dsd = {}\n    for split in sorted(df[\"split\"].unique()):\n        sdf = df[df[\"split\"] == split].copy()\n        keep = [\"audio_path\",\"text\",\"text_norm\",\"speaker_id\"]\n        for extra in [\"duration_sec\",\"quality_score\",\"curriculum_bin\"]:\n            if extra in sdf.columns:\n                keep.append(extra)\n        dsd[split] = Dataset.from_pandas(sdf[keep], preserve_index=False)\n    return DatasetDict(dsd)\n\nls_ds = to_hf_dataset(ls_df_f[ls_df_f[\"split\"].isin([\"train\",\"dev\",\"test\"])])\ncv_ds = to_hf_dataset(cv_df_f[cv_df_f[\"split\"].isin([\"train\",\"dev\",\"test\"])])\n\nprint(ls_ds)\nprint(cv_ds)\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:51:50.104350Z","iopub.execute_input":"2026-02-19T23:51:50.104664Z","iopub.status.idle":"2026-02-19T23:51:50.527600Z","shell.execute_reply.started":"2026-02-19T23:51:50.104641Z","shell.execute_reply":"2026-02-19T23:51:50.526876Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DatasetDict({\n    dev: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 2311\n    })\n    test: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 2382\n    })\n    train: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 40356\n    })\n})\nDatasetDict({\n    dev: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 2251\n    })\n    test: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 1149\n    })\n    train: Dataset({\n        features: ['audio_path', 'text', 'text_norm', 'speaker_id', 'duration_sec', 'quality_score', 'curriculum_bin'],\n        num_rows: 23658\n    })\n})\n","output_type":"stream"}],"execution_count":11},{"id":"80de2489-4965-4382-9a27-e32d3d8086f3","cell_type":"markdown","source":"## 9) Processor, tokenization, and SpecAugment (optional)","metadata":{}},{"id":"63316305-c7d4-4684-bc6f-2184f5e8bcd9","cell_type":"code","source":"# Cell 11 — Whisper processor (stable prompt IDs) + tokenizer settings\n\nfrom transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(CFG[\"MODEL_NAME\"])\nfeature_extractor = processor.feature_extractor\ntokenizer = processor.tokenizer\n\n# Build stable forced decoder prompt IDs (language/task tokens)\nFORCED_DECODER_IDS = None\ntry:\n    FORCED_DECODER_IDS = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n    print(\"✅ forced_decoder_ids ready:\", FORCED_DECODER_IDS[:3], \"...\")\nexcept Exception as e:\n    print(\"⚠️ Could not set forced_decoder_ids automatically:\", e)\n\nprint(\"Tokenizer vocab:\", tokenizer.vocab_size)\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:51:53.926016Z","iopub.execute_input":"2026-02-19T23:51:53.926334Z","iopub.status.idle":"2026-02-19T23:51:55.431484Z","shell.execute_reply.started":"2026-02-19T23:51:53.926307Z","shell.execute_reply":"2026-02-19T23:51:55.430555Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7c3596638b49038e177c55176ab104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2af337b83a4f4d870e83c349804906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f848518f8ca24d9c9f435bd114891718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e59ada4f3f846bc8ab37dcb7cc1f115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558ecf712d6141eb8bac5409f49dd41b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"579bbb83ff424580b120d538d52afc45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27116a11aba44b4cae664d39e70c483b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20694474e23f4a04bc537da368c5959d"}},"metadata":{}},{"name":"stdout","text":"✅ forced_decoder_ids ready: [(1, 50258), (2, 50358), (3, 50362)] ...\nTokenizer vocab: 50257\n","output_type":"stream"}],"execution_count":12},{"id":"a12266f0-717c-47c3-84dc-7d459ecdd5c4","cell_type":"markdown","source":"## 10) Data collator (with optional SpecAugment during training)","metadata":{}},{"id":"82ea7040-698b-4c64-8262-b35db01c6738","cell_type":"code","source":"# Cell — Collator (P100/Kaggle safe, self-contained)\n# Handles:\n# 1) list[dict]\n# 2) dict[list]\n# audio_path as either:\n#  - string file path\n#  - HF Audio dict {\"array\":..., \"sampling_rate\":...}\n# Applies SpecAugment ONLY when model is in training mode.\n\nimport numpy as np\nimport torch\nimport torchaudio\nimport librosa\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n# --- SpecAugment transforms (defined here to avoid NameError) ---\ntime_mask = torchaudio.transforms.TimeMasking(time_mask_param=30)\nfreq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=10)\n\ndef load_audio_16k(item):\n    # item can be Audio dict or file path string\n    if isinstance(item, dict) and \"array\" in item and \"sampling_rate\" in item:\n        y = item[\"array\"]\n        sr = int(item[\"sampling_rate\"])\n        if sr != 16000:\n            y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n        return y.astype(np.float32), 16000\n\n    if isinstance(item, str):\n        path = item\n        try:\n            wav, sr = torchaudio.load(path)\n            y = wav.mean(dim=0).numpy()\n        except Exception:\n            y, sr = librosa.load(path, sr=None, mono=True)\n\n        if sr != 16000:\n            y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n        return y.astype(np.float32), 16000\n\n    raise TypeError(f\"Unsupported audio item type: {type(item)}\")\n\ndef _ensure_list_of_dicts(features: Union[List[Dict[str, Any]], Dict[str, List[Any]]]):\n    # If HF dataset returns dict-of-lists, convert to list-of-dicts\n    if isinstance(features, dict):\n        keys = list(features.keys())\n        n = len(features[keys[0]]) if keys else 0\n        return [{k: features[k][i] for k in keys} for i in range(n)]\n    return features  # already list-of-dicts\n\n@dataclass\nclass WhisperCollatorOnTheFly:\n    processor: Any\n    model_ref: Any = None\n    use_specaug: bool = False\n    time_mask: Any = None\n    freq_mask: Any = None\n\n    def __call__(self, features):\n        features = _ensure_list_of_dicts(features)\n\n        waves = []\n        for f in features:\n            y, _ = load_audio_16k(f[\"audio_path\"])\n            waves.append(y)\n\n        inputs = self.processor.feature_extractor(\n            waves, sampling_rate=16000, return_tensors=\"pt\"\n        )\n        input_features = inputs[\"input_features\"]\n\n        # SpecAugment only during training (Trainer uses ONE collator for train+eval)\n        is_training = (self.model_ref is not None and self.model_ref.training)\n        if self.use_specaug and is_training and self.time_mask is not None and self.freq_mask is not None:\n            x = input_features\n            x = self.time_mask(x)\n            x = self.freq_mask(x)\n            input_features = x\n\n        texts = [f.get(\"text_norm\", f[\"text\"]) for f in features]\n        tok = self.processor.tokenizer(texts, padding=True, return_tensors=\"pt\")\n        labels = tok[\"input_ids\"].masked_fill(tok[\"attention_mask\"].ne(1), -100)\n\n        return {\"input_features\": input_features, \"labels\": labels}\n\n# ✅ Single collator used by Trainer for both train and eval\ntrain_collator = WhisperCollatorOnTheFly(\n    processor=processor,\n    model_ref=None,  # attach after trainer is created\n    use_specaug=bool(CFG.get(\"USE_SPECAUG\", False)),\n    time_mask=time_mask if CFG.get(\"USE_SPECAUG\", False) else None,\n    freq_mask=freq_mask if CFG.get(\"USE_SPECAUG\", False) else None,\n)\n\nprint(\"✅ Collator ready (Trainer will reuse this for eval too).\")","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:57:38.206920Z","iopub.execute_input":"2026-02-19T23:57:38.207673Z","iopub.status.idle":"2026-02-19T23:57:38.222955Z","shell.execute_reply.started":"2026-02-19T23:57:38.207643Z","shell.execute_reply":"2026-02-19T23:57:38.222135Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Collator ready (Trainer will reuse this for eval too).\n","output_type":"stream"}],"execution_count":15},{"id":"105c7546-cb38-4bb6-a4c3-f24152e4555b","cell_type":"markdown","source":"## 11) Speaker‑balanced sampling (WeightedRandomSampler) via custom Trainer","metadata":{}},{"id":"f1d7f986-c45b-4cd0-83ba-40130db28eae","cell_type":"code","source":"# Cell 13 — Custom trainer: speaker-balanced sampling + separate eval collator (no SpecAugment)\n\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\nclass SpeakerBalancedSeq2SeqTrainer(Seq2SeqTrainer):\n    def __init__(self, *args, speaker_balanced=False, eval_collator=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.speaker_balanced = speaker_balanced\n        self.eval_collator = eval_collator\n\n    def get_train_dataloader(self):\n        if not self.speaker_balanced or not CFG[\"USE_SPEAKER_BALANCED_SAMPLING\"]:\n            return super().get_train_dataloader()\n\n        train_dataset = self.train_dataset\n        spk = [str(s) for s in train_dataset[\"speaker_id\"]]\n        freq = {}\n        for s in spk:\n            freq[s] = freq.get(s, 0) + 1\n        weights = torch.DoubleTensor([1.0 / freq[s] for s in spk])\n        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n        return DataLoader(\n            train_dataset,\n            batch_size=self.args.per_device_train_batch_size,\n            sampler=sampler,\n            collate_fn=self.data_collator,\n            num_workers=0,\n            pin_memory=False,\n        )\n\n    def get_eval_dataloader(self, eval_dataset=None):\n        dl = super().get_eval_dataloader(eval_dataset)\n        if self.eval_collator is None:\n            return dl\n        # Rebuild DataLoader with eval_collator (keeps everything else identical)\n        return DataLoader(\n            dl.dataset,\n            batch_size=dl.batch_size,\n            sampler=dl.sampler,\n            collate_fn=self.eval_collator,\n            num_workers=0,\n            pin_memory=False,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:57:44.158598Z","iopub.execute_input":"2026-02-19T23:57:44.158962Z","iopub.status.idle":"2026-02-19T23:57:44.167341Z","shell.execute_reply.started":"2026-02-19T23:57:44.158930Z","shell.execute_reply":"2026-02-19T23:57:44.166622Z"},"trusted":true},"outputs":[],"execution_count":16},{"id":"2de56724-7e50-462d-8314-1a2572cc370c","cell_type":"markdown","source":"## 12) Metrics (WER/CER) with decode params + normalization","metadata":{}},{"id":"89f88cf7-6145-46a1-99f2-d6329817f2ca","cell_type":"code","source":"# Cell 14 — Compute metrics (WER/CER)\nimport evaluate\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n\ndef postprocess_text(preds, labels):\n    preds = [normalize_text(p) for p in preds]\n    labels = [normalize_text(l) for l in labels]\n    return preds, labels\n\ndef make_compute_metrics():\n    def compute_metrics(pred):\n        pred_ids = pred.predictions\n        label_ids = pred.label_ids\n        label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n        pred_str, label_str = postprocess_text(pred_str, label_str)\n\n        return {\n            \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n            \"cer\": cer_metric.compute(predictions=pred_str, references=label_str),\n        }\n    return compute_metrics\n","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:57:48.401024Z","iopub.execute_input":"2026-02-19T23:57:48.401757Z","iopub.status.idle":"2026-02-19T23:57:49.692237Z","shell.execute_reply.started":"2026-02-19T23:57:48.401726Z","shell.execute_reply":"2026-02-19T23:57:49.691631Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ff8b0705fb4f91ad006fe3252aed5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abda6e9303634ed4a1ed7edc241383d0"}},"metadata":{}}],"execution_count":17},{"id":"49682d9a-6034-4347-ae32-4f6ac3c599d3","cell_type":"code","source":"# Disable bitsandbytes completely to avoid triton.ops issues\n!pip -q uninstall -y bitsandbytes\n\nimport os\nos.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2026-02-19T16:33:29.040752Z","iopub.execute_input":"2026-02-19T16:33:29.041077Z","iopub.status.idle":"2026-02-19T16:33:30.333912Z","shell.execute_reply.started":"2026-02-19T16:33:29.041049Z","shell.execute_reply":"2026-02-19T16:33:30.333161Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":36},{"id":"f865a2b3-f256-4678-8f74-dc8cb118c33f","cell_type":"code","source":"CFG[\"USE_8BIT_OPTIM\"] = False","metadata":{"execution":{"iopub.status.busy":"2026-02-19T23:58:00.926547Z","iopub.execute_input":"2026-02-19T23:58:00.926896Z","iopub.status.idle":"2026-02-19T23:58:00.931633Z","shell.execute_reply.started":"2026-02-19T23:58:00.926867Z","shell.execute_reply":"2026-02-19T23:58:00.930738Z"},"trusted":true},"outputs":[],"execution_count":18},{"id":"05472435-ef3b-4efc-ba52-c7c0bf0b4be8","cell_type":"code","source":"import inspect\nfrom transformers import WhisperForConditionalGeneration\nfrom peft import LoraConfig, get_peft_model\n\n# Build a set of allowed kwargs for Whisper forward\n_WHISPER_FORWARD_KEYS = set(inspect.signature(WhisperForConditionalGeneration.forward).parameters.keys())\n\nclass WhisperForConditionalGenerationCompat(WhisperForConditionalGeneration):\n    \"\"\"\n    Makes Whisper compatible with Trainer/PEFT extras:\n      - PEFT may call forward(input_ids=...)\n      - Trainer may pass num_items_in_batch\n      - Filters unknown kwargs safely\n    \"\"\"\n    def forward(self, *args, **kwargs):\n        # 1) Drop Trainer bookkeeping kwarg\n        kwargs.pop(\"num_items_in_batch\", None)\n\n        # 2) PEFT sometimes uses input_ids for encoder inputs (Whisper expects input_features)\n        if \"input_features\" not in kwargs and \"input_ids\" in kwargs:\n            kwargs[\"input_features\"] = kwargs.pop(\"input_ids\")\n\n        # 3) Whisper doesn't use inputs_embeds; remove if present\n        kwargs.pop(\"inputs_embeds\", None)\n\n        # 4) Filter any other unexpected kwargs defensively\n        kwargs = {k: v for k, v in kwargs.items() if k in _WHISPER_FORWARD_KEYS}\n\n        return super().forward(*args, **kwargs)\n\ndef build_model():\n    # Keep your P100-safe attention if you were using it\n    try:\n        model = WhisperForConditionalGenerationCompat.from_pretrained(\n            CFG[\"MODEL_NAME\"],\n            attn_implementation=\"eager\",\n        )\n    except TypeError:\n        model = WhisperForConditionalGenerationCompat.from_pretrained(CFG[\"MODEL_NAME\"])\n        if hasattr(model.config, \"attn_implementation\"):\n            model.config.attn_implementation = \"eager\"\n\n    model.config.forced_decoder_ids = None\n    model.config.suppress_tokens = []\n    model.generation_config.max_new_tokens = CFG[\"GEN_MAX_NEW_TOKENS\"]\n    model.generation_config.num_beams = CFG[\"GEN_BEAMS\"]\n\n    model.gradient_checkpointing_enable()\n    model.config.use_cache = False\n\n    if CFG[\"USE_LORA\"]:\n        lora_cfg = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"SEQ_2_SEQ_LM\",\n            target_modules=[\"q_proj\", \"v_proj\"],\n        )\n        model = get_peft_model(model, lora_cfg)\n        model.print_trainable_parameters()\n\n    return model\n\nmodel = build_model()\nprint(\"✅ Model rebuilt with WhisperForConditionalGenerationCompat\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T00:40:07.451115Z","iopub.execute_input":"2026-02-20T00:40:07.451964Z","iopub.status.idle":"2026-02-20T00:40:08.095116Z","shell.execute_reply.started":"2026-02-20T00:40:07.451923Z","shell.execute_reply":"2026-02-20T00:40:08.094227Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,769,472 || all params: 243,503,616 || trainable%: 0.7267\n✅ Model rebuilt with WhisperForConditionalGenerationCompat\n","output_type":"stream"}],"execution_count":33},{"id":"f9104cfa-23a6-47d5-9e94-7dedfdd7eccc","cell_type":"markdown","source":"## 13) Model init (LoRA + 8‑bit optimizer) — Stage A: LibriSpeech","metadata":{}},{"id":"ac7c5131-ee96-41a7-a54a-1fcd054c1c03","cell_type":"code","source":"# Cell 15 — Load Whisper + attach LoRA (force eager attention for P100)\n\nfrom transformers import WhisperForConditionalGeneration\n\nclass WhisperForConditionalGenerationCompat(WhisperForConditionalGeneration):\n    \"\"\"\n    Makes Whisper compatible with:\n      - PEFT passing input_ids\n      - Trainer passing num_items_in_batch (and other extra kwargs)\n    \"\"\"\n    def forward(\n        self,\n        input_features=None,\n        input_ids=None,\n        attention_mask=None,\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        encoder_outputs=None,\n        past_key_values=None,\n        decoder_inputs_embeds=None,\n        labels=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        inputs_embeds=None,   # sometimes passed by wrappers; Whisper doesn't use it\n        **kwargs\n    ):\n        # ✅ Trainer sometimes passes this (Whisper doesn't accept it)\n        kwargs.pop(\"num_items_in_batch\", None)\n\n        # ✅ Some stacks pass other harmless extras; ignore them safely\n        kwargs.pop(\"output_router_logits\", None)\n        kwargs.pop(\"return_loss\", None)\n\n        # ✅ PEFT wrapper often calls base_model(input_ids=...)\n        if input_features is None and input_ids is not None:\n            input_features = input_ids\n\n        return super().forward(\n            input_features=input_features,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            encoder_outputs=encoder_outputs,\n            past_key_values=past_key_values,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )","metadata":{"execution":{"iopub.status.busy":"2026-02-20T00:40:13.298594Z","iopub.execute_input":"2026-02-20T00:40:13.299132Z","iopub.status.idle":"2026-02-20T00:40:13.312773Z","shell.execute_reply.started":"2026-02-20T00:40:13.299073Z","shell.execute_reply":"2026-02-20T00:40:13.311793Z"},"trusted":true},"outputs":[],"execution_count":34},{"id":"2e22a93d-4121-44a5-8084-a1a84c348903","cell_type":"markdown","source":"## 14) Training arguments (P100‑safe) + Stage A trainer","metadata":{}},{"id":"016fe592-4842-46f3-a1e8-58b78f832a65","cell_type":"code","source":"# Cell 16 — Stage A args + Trainer (self-healing if model not defined)\n\nimport os\nfrom transformers import Seq2SeqTrainingArguments\n\n# ---- 1) Ensure we have a model ----\nif \"model\" not in globals() or model is None:\n    print(\"⚠️ model not found in memory — rebuilding from base checkpoint...\")\n\n    # You should already have build_model() defined in your model cell.\n    # If not, re-run the model cell first.\n    model = build_model()\n\n# ---- 2) Use P100-safe optimizer ----\noptim_name = \"adamw_torch\"\n\nstage_a_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/asr_runs/stageA_librispeech\",\n    per_device_train_batch_size=CFG[\"PER_DEVICE_TRAIN_BS\"],\n    per_device_eval_batch_size=CFG[\"PER_DEVICE_EVAL_BS\"],\n    gradient_accumulation_steps=CFG[\"GRAD_ACCUM_STEPS\"],\n    learning_rate=CFG[\"STAGE_A_LR\"],\n    warmup_steps=100,\n    max_steps=CFG[\"STAGE_A_MAX_STEPS\"],\n    fp16=True,\n    bf16=False,\n    optim=optim_name,\n    dataloader_num_workers=0,\n    logging_steps=50,\n    eval_strategy=\"steps\",   # <-- correct arg name\n    eval_steps=200,\n    save_steps=200,\n    save_total_limit=1,\n    predict_with_generate=True,\n    generation_num_beams=CFG[\"GEN_BEAMS\"],\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n)\n\n# ---- 3) Make sure we use the LIGHT datasets (audio_path/text columns) ----\n# If you use ls_ds_light, use it. Otherwise fallback to ls_ds.\ntrain_ds = ls_ds_light[\"train\"] if \"ls_ds_light\" in globals() else ls_ds[\"train\"]\ndev_ds   = ls_ds_light[\"dev\"]   if \"ls_ds_light\" in globals() else ls_ds[\"dev\"]\n\ntrainer_a = SpeakerBalancedSeq2SeqTrainer(\n    model=model,\n    args=stage_a_args,\n    train_dataset=train_ds,\n    eval_dataset=dev_ds,\n    data_collator=train_collator,\n    compute_metrics=make_compute_metrics(),\n    tokenizer=processor.feature_extractor,\n    speaker_balanced=CFG[\"USE_SPEAKER_BALANCED_SAMPLING\"],\n)\n\n# Attach model to collator so SpecAugment toggles correctly\ntry:\n    train_collator.model_ref = trainer_a.model\nexcept Exception:\n    pass\n\nprint(\"Stage A ready ✅\")","metadata":{"execution":{"iopub.status.busy":"2026-02-20T00:40:36.555300Z","iopub.execute_input":"2026-02-20T00:40:36.555668Z","iopub.status.idle":"2026-02-20T00:40:37.002047Z","shell.execute_reply.started":"2026-02-20T00:40:36.555626Z","shell.execute_reply":"2026-02-20T00:40:37.001178Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_149/1066523074.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpeakerBalancedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Stage A ready ✅\n","output_type":"stream"}],"execution_count":35},{"id":"1f9bbf21-7f60-4c32-a791-f682e1941e33","cell_type":"code","source":"import torch\nprint(\"torch:\", torch.__version__)\nprint(\"cuda runtime:\", torch.version.cuda)\nprint(\"gpu:\", torch.cuda.get_device_name(0))\nprint(\"capability:\", torch.cuda.get_device_capability(0))\nprint(\"arch list in this torch build:\", torch.cuda.get_arch_list())\n\n# quick kernel test (this is what currently fails for you)\nx = torch.zeros((2,3), device=\"cuda\")\nprint(\"cuda kernel OK ✅\", x.shape)","metadata":{"execution":{"iopub.status.busy":"2026-02-20T00:40:40.764605Z","iopub.execute_input":"2026-02-20T00:40:40.764969Z","iopub.status.idle":"2026-02-20T00:40:40.771224Z","shell.execute_reply.started":"2026-02-20T00:40:40.764939Z","shell.execute_reply":"2026-02-20T00:40:40.770284Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch: 2.8.0+cu126\ncuda runtime: 12.6\ngpu: Tesla P100-PCIE-16GB\ncapability: (6, 0)\narch list in this torch build: ['sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90']\ncuda kernel OK ✅ torch.Size([2, 3])\n","output_type":"stream"}],"execution_count":36},{"id":"5aad32cf-cd8c-4c76-bb5c-4f6a1ffe9a79","cell_type":"markdown","source":"## 15) Run Stage A training (LibriSpeech)","metadata":{}},{"id":"2f967ae1-50db-47fa-b8b9-396fcdcfe188","cell_type":"code","source":"# One-batch GPU debug\nbatch = next(iter(trainer_a.get_train_dataloader()))\nbatch = {k: (v.cuda() if hasattr(v, \"cuda\") else v) for k, v in batch.items()}\n\nmodel = trainer_a.model.cuda()\nmodel.train()\n\nout = model(**batch)\nprint(\"Forward OK ✅\", out.loss.item())\n\nout.loss.backward()\nprint(\"Backward OK ✅\")","metadata":{"execution":{"iopub.status.busy":"2026-02-20T00:40:44.510730Z","iopub.execute_input":"2026-02-20T00:40:44.511457Z","iopub.status.idle":"2026-02-20T00:40:44.904469Z","shell.execute_reply.started":"2026-02-20T00:40:44.511426Z","shell.execute_reply":"2026-02-20T00:40:44.903561Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Forward OK ✅ 1.5956324338912964\nBackward OK ✅\n","output_type":"stream"}],"execution_count":37},{"id":"1b40affb-8f1d-47f2-a0cb-6f7b33ee4e18","cell_type":"code","source":"import inspect\nfrom accelerate import Accelerator\n\n_sig = inspect.signature(Accelerator.unwrap_model)\nif \"keep_torch_compile\" not in _sig.parameters:\n    _orig_unwrap_model = Accelerator.unwrap_model\n\n    def _unwrap_model_compat(self, model, *args, **kwargs):\n        # transformers may pass these on newer versions\n        kwargs.pop(\"keep_torch_compile\", None)\n        kwargs.pop(\"recursive\", None)\n\n        # Call the original unwrap_model with only supported kwargs\n        orig_sig = inspect.signature(_orig_unwrap_model)\n        filtered = {k: v for k, v in kwargs.items() if k in orig_sig.parameters}\n\n        return _orig_unwrap_model(self, model, *args, **filtered)\n\n    Accelerator.unwrap_model = _unwrap_model_compat\n    print(\"✅ Patched accelerate.Accelerator.unwrap_model for keep_torch_compile compatibility.\")\nelse:\n    print(\"✅ accelerate already supports keep_torch_compile.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T00:40:48.265759Z","iopub.execute_input":"2026-02-20T00:40:48.266127Z","iopub.status.idle":"2026-02-20T00:40:48.273196Z","shell.execute_reply.started":"2026-02-20T00:40:48.266096Z","shell.execute_reply":"2026-02-20T00:40:48.272230Z"}},"outputs":[{"name":"stdout","text":"✅ Patched accelerate.Accelerator.unwrap_model for keep_torch_compile compatibility.\n","output_type":"stream"}],"execution_count":38},{"id":"8338ff86-028a-44e9-85cd-f7eab091debb","cell_type":"code","source":"import inspect\nfrom accelerate import Accelerator\nfrom accelerate.utils import extract_model_from_parallel\n\ndef unwrap_model_no_recursion(self, model, *args, **kwargs):\n    # transformers may pass these on newer versions\n    kwargs.pop(\"keep_torch_compile\", None)\n    kwargs.pop(\"recursive\", None)\n\n    # forward only the kwargs that extract_model_from_parallel supports\n    sig = inspect.signature(extract_model_from_parallel)\n    filtered = {k: v for k, v in kwargs.items() if k in sig.parameters}\n\n    return extract_model_from_parallel(model, **filtered)\n\n# Apply patch safely (idempotent)\nif not getattr(Accelerator.unwrap_model, \"_no_recursion_patch\", False):\n    Accelerator.unwrap_model = unwrap_model_no_recursion\n    Accelerator.unwrap_model._no_recursion_patch = True\n    print(\"✅ Patched Accelerator.unwrap_model (no recursion, keep_torch_compile ignored).\")\nelse:\n    print(\"✅ unwrap_model already patched (no recursion).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T00:42:24.678284Z","iopub.execute_input":"2026-02-20T00:42:24.678663Z","iopub.status.idle":"2026-02-20T00:42:24.685864Z","shell.execute_reply.started":"2026-02-20T00:42:24.678634Z","shell.execute_reply":"2026-02-20T00:42:24.684911Z"}},"outputs":[{"name":"stdout","text":"✅ Patched Accelerator.unwrap_model (no recursion, keep_torch_compile ignored).\n","output_type":"stream"}],"execution_count":40},{"id":"128ccb37-774d-404b-a6a8-23e6cb60e410","cell_type":"code","source":"import types\n\ndef patch_generate_drop_labels(model):\n    \"\"\"\n    Patch model.generate to ignore labels so Seq2SeqTrainer evaluation won't crash.\n    Stores patch state on the model object (safe on Kaggle).\n    \"\"\"\n    if getattr(model, \"_drop_labels_patched\", False):\n        return\n\n    orig_generate = model.generate\n\n    def generate_no_labels(self, *args, **kwargs):\n        kwargs.pop(\"labels\", None)\n        kwargs.pop(\"label\", None)\n        # don't forward decoder_input_ids for whisper generation\n        kwargs.pop(\"decoder_input_ids\", None)\n        return orig_generate(*args, **kwargs)\n\n    model.generate = types.MethodType(generate_no_labels, model)\n    model._drop_labels_patched = True\n\n# Patch trainer model\npatch_generate_drop_labels(trainer_a.model)\n\n# If PEFT-wrapped, patch its base model too (different wrappers use different attrs)\nfor attr in [\"base_model\", \"model\"]:\n    if hasattr(trainer_a.model, attr):\n        try:\n            patch_generate_drop_labels(getattr(trainer_a.model, attr))\n        except Exception:\n            pass\n\nprint(\"✅ Patched generate() to drop labels during decoding.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T00:50:15.374003Z","iopub.execute_input":"2026-02-20T00:50:15.374890Z","iopub.status.idle":"2026-02-20T00:50:15.382599Z","shell.execute_reply.started":"2026-02-20T00:50:15.374831Z","shell.execute_reply":"2026-02-20T00:50:15.381675Z"}},"outputs":[{"name":"stdout","text":"✅ Patched generate() to drop labels during decoding.\n","output_type":"stream"}],"execution_count":43},{"id":"3d67efc8-4f91-4274-a5d5-4620a9ecb94b","cell_type":"code","source":"# Cell 17 — Train Stage A\ntrain_result_a = trainer_a.train()\nmetrics_a = trainer_a.evaluate()\n\nprint(\"Stage A eval:\", metrics_a)\ntrainer_a.save_model()\npd.DataFrame([metrics_a]).to_csv(\"/kaggle/working/stageA_eval_metrics.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2026-02-20T00:50:19.586272Z","iopub.execute_input":"2026-02-20T00:50:19.587072Z","iopub.status.idle":"2026-02-20T04:50:52.051411Z","shell.execute_reply.started":"2026-02-20T00:50:19.587042Z","shell.execute_reply":"2026-02-20T04:50:52.050239Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 3:29:57, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2.061500</td>\n      <td>0.749182</td>\n      <td>0.122979</td>\n      <td>0.113462</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.125700</td>\n      <td>0.741068</td>\n      <td>0.082969</td>\n      <td>0.063727</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.047800</td>\n      <td>0.739126</td>\n      <td>0.077120</td>\n      <td>0.064710</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.062800</td>\n      <td>0.738254</td>\n      <td>0.068543</td>\n      <td>0.053975</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.035100</td>\n      <td>0.737125</td>\n      <td>0.068764</td>\n      <td>0.052112</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.110600</td>\n      <td>0.736806</td>\n      <td>0.070116</td>\n      <td>0.054948</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1156' max='1156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1156/1156 28:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Stage A eval: {'eval_loss': 0.7382535934448242, 'eval_wer': 0.06854263946915704, 'eval_cer': 0.05397487973344814, 'eval_runtime': 1833.2373, 'eval_samples_per_second': 1.261, 'eval_steps_per_second': 0.631, 'epoch': 0.23788284269997026}\n","output_type":"stream"}],"execution_count":44},{"id":"e72c5ec7-e987-418f-9144-dc5bd7efee91","cell_type":"markdown","source":"## 16) Confidence-based transcript denoise (teacher-forced loss on CV train)","metadata":{}},{"id":"6b78a782-378f-4dd1-bfef-3bdb381df7ec","cell_type":"code","source":"eval_collator = WhisperCollatorOnTheFly(\n    processor=processor,\n    use_specaug=False,\n    time_mask=None,\n    freq_mask=None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:52:26.466646Z","iopub.execute_input":"2026-02-20T04:52:26.467030Z","iopub.status.idle":"2026-02-20T04:52:26.471216Z","shell.execute_reply.started":"2026-02-20T04:52:26.466983Z","shell.execute_reply":"2026-02-20T04:52:26.470382Z"}},"outputs":[],"execution_count":46},{"id":"80569838-bb10-4bf2-922f-fe3f46eea639","cell_type":"code","source":"# Cell 18 — Score CV samples by teacher-forced loss (subset) and drop worst fraction\n@torch.no_grad()\ndef score_dataset_by_loss(trainer: Seq2SeqTrainer, ds: Dataset, max_samples: int):\n    model = trainer.model\n    model.eval()\n    ds_small = ds.select(range(min(max_samples, len(ds))))\n    losses = []\n    for i in range(0, len(ds_small), CFG[\"PER_DEVICE_EVAL_BS\"]):\n        batch = [ds_small[j] for j in range(i, min(i+CFG[\"PER_DEVICE_EVAL_BS\"], len(ds_small)))]\n        batch_t = eval_collator(batch)\n        batch_t = {k: v.to(model.device) for k, v in batch_t.items()}\n        out = model(**batch_t)\n        losses.append(out.loss.detach().float().cpu().item())\n\n    per_sample = []\n    for b, loss in enumerate(losses):\n        start = b * CFG[\"PER_DEVICE_EVAL_BS\"]\n        end = min(start + CFG[\"PER_DEVICE_EVAL_BS\"], len(ds_small))\n        per_sample.extend([loss] * (end - start))\n    return np.array(per_sample)\n\ncv_train = cv_ds[\"train\"]\n\nif CFG[\"USE_TRANSCRIPT_DENOISE\"]:\n    print(\"Scoring CV train for denoise (subset)...\")\n    loss_scores = score_dataset_by_loss(trainer_a, cv_train, CFG[\"DENOISE_SCORE_MAX_SAMPLES\"])\n    cutoff = np.quantile(loss_scores, 1.0 - CFG[\"DENOISE_DROP_FRACTION\"])\n    keep_mask = loss_scores <= cutoff\n    kept_indices = list(np.where(keep_mask)[0])\n    if len(cv_train) > len(loss_scores):\n        kept_indices += list(range(len(loss_scores), len(cv_train)))\n    cv_train_denoised = cv_train.select(kept_indices)\n    print(\"CV train before:\", len(cv_train), \"after denoise:\", len(cv_train_denoised))\nelse:\n    cv_train_denoised = cv_train\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:52:31.949363Z","iopub.execute_input":"2026-02-20T04:52:31.949688Z","iopub.status.idle":"2026-02-20T05:09:44.683036Z","shell.execute_reply.started":"2026-02-20T04:52:31.949659Z","shell.execute_reply":"2026-02-20T05:09:44.682094Z"}},"outputs":[{"name":"stdout","text":"Scoring CV train for denoise (subset)...\nCV train before: 23658 after denoise: 22698\n","output_type":"stream"}],"execution_count":47},{"id":"5e8c2aaa-ad1b-439d-9d7f-42af8279bd9e","cell_type":"markdown","source":"## 17) Stage B: Adapt on Common Voice","metadata":{}},{"id":"77feef1d-6bbd-4abe-aa65-7b372ae4888e","cell_type":"code","source":"# Cell 19 — Stage B trainer (Common Voice adaptation)\nstage_b_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/asr_runs/stageB_commonvoice\",\n    per_device_train_batch_size=CFG[\"PER_DEVICE_TRAIN_BS\"],\n    per_device_eval_batch_size=CFG[\"PER_DEVICE_EVAL_BS\"],\n    gradient_accumulation_steps=CFG[\"GRAD_ACCUM_STEPS\"],\n    learning_rate=CFG[\"STAGE_B_LR\"],\n    warmup_steps=80,\n    max_steps=CFG[\"STAGE_B_MAX_STEPS\"],\n    fp16=True,\n    optim=optim_name,\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    save_steps=200,\n    save_total_limit=1,\n    predict_with_generate=True,\n    generation_num_beams=CFG[\"GEN_BEAMS\"],\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    dataloader_num_workers=0,\n\n)\n\ntrainer_b = SpeakerBalancedSeq2SeqTrainer(\n    model=model,\n    args=stage_b_args,\n    train_dataset=cv_ds[\"train\"],\n    eval_dataset=cv_ds[\"dev\"],\n    data_collator=train_collator,\n    compute_metrics=make_compute_metrics(),\n    tokenizer=processor.feature_extractor,\n    speaker_balanced=CFG[\"USE_SPEAKER_BALANCED_SAMPLING\"],\n)\n\ntrain_collator.model_ref = trainer_b.model\n\n\nprint(\"Stage B ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T05:10:00.006550Z","iopub.execute_input":"2026-02-20T05:10:00.007397Z","iopub.status.idle":"2026-02-20T05:10:00.058708Z","shell.execute_reply.started":"2026-02-20T05:10:00.007347Z","shell.execute_reply":"2026-02-20T05:10:00.057962Z"}},"outputs":[{"name":"stdout","text":"Stage B ready.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_149/1066523074.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SpeakerBalancedSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":48},{"id":"9057f283-3786-476a-950c-81a90606a30d","cell_type":"markdown","source":"## 18) Run Stage B training + cross-domain evaluation","metadata":{}},{"id":"182aaa43-4400-4b33-9618-3d2faf3aced5","cell_type":"code","source":"# Cell 20 — Train Stage B + evaluate on CV test and LS test (use *_ds_light)\n\ntrain_result_b = trainer_b.train()\n\nmetrics_b_dev     = trainer_b.evaluate(eval_dataset=cv_ds[\"dev\"])\nmetrics_b_cv_test = trainer_b.evaluate(eval_dataset=cv_ds[\"test\"])\nmetrics_b_ls_test = trainer_b.evaluate(eval_dataset=ls_ds[\"test\"])\n\nprint(\"Stage B dev:\", metrics_b_dev)\nprint(\"Stage B CV test:\", metrics_b_cv_test)\nprint(\"Stage B LS test:\", metrics_b_ls_test)\n\ntrainer_b.save_model()\npd.DataFrame([metrics_b_dev]).to_csv(\"/kaggle/working/stageB_dev_metrics.csv\", index=False)\npd.DataFrame([metrics_b_cv_test]).to_csv(\"/kaggle/working/stageB_cv_test_metrics.csv\", index=False)\npd.DataFrame([metrics_b_ls_test]).to_csv(\"/kaggle/working/stageB_ls_test_metrics.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T05:10:13.550673Z","iopub.execute_input":"2026-02-20T05:10:13.551402Z","iopub.status.idle":"2026-02-20T07:52:23.103287Z","shell.execute_reply.started":"2026-02-20T05:10:13.551370Z","shell.execute_reply":"2026-02-20T07:52:23.102364Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 1:40:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>4.627200</td>\n      <td>1.088703</td>\n      <td>0.127312</td>\n      <td>0.072924</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.616400</td>\n      <td>1.084137</td>\n      <td>0.120381</td>\n      <td>0.070271</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.648300</td>\n      <td>1.082327</td>\n      <td>0.115703</td>\n      <td>0.067240</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>4.348600</td>\n      <td>1.081868</td>\n      <td>0.112237</td>\n      <td>0.063975</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19087' max='1126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1126/1126 4:39:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Stage B dev: {'eval_loss': 1.0818679332733154, 'eval_wer': 0.11223738358241282, 'eval_cer': 0.06397522317361996, 'eval_runtime': 1224.3671, 'eval_samples_per_second': 1.839, 'eval_steps_per_second': 0.92, 'epoch': 0.3043367993913264}\nStage B CV test: {'eval_loss': 1.1138039827346802, 'eval_wer': 0.13712219543853144, 'eval_cer': 0.06211715005280689, 'eval_runtime': 638.4521, 'eval_samples_per_second': 1.8, 'eval_steps_per_second': 0.901, 'epoch': 0.3043367993913264}\nStage B LS test: {'eval_loss': 0.7851576805114746, 'eval_wer': 0.11142461142461142, 'eval_cer': 0.05782289048936061, 'eval_runtime': 1833.0052, 'eval_samples_per_second': 1.3, 'eval_steps_per_second': 0.65, 'epoch': 0.3043367993913264}\n","output_type":"stream"}],"execution_count":49},{"id":"599c7130-0a74-4f17-aa2e-1c9c0c730258","cell_type":"markdown","source":"## 19) Decode-time tuning (cheap WER/CER improvements)","metadata":{}},{"id":"c0e0d990-ef75-40ac-bbd0-39ded30752e2","cell_type":"code","source":"# Cell 21 — Decode tuning grid on CV dev, then report best on CV test\nfrom itertools import product\n\ndef eval_with_gen_kwargs(trainer: Seq2SeqTrainer, ds: Dataset, gen_kwargs: dict):\n    old_beams = trainer.args.generation_num_beams\n    trainer.args.generation_num_beams = gen_kwargs.get(\"num_beams\", CFG[\"GEN_BEAMS\"])\n    trainer.model.generation_config.num_beams = gen_kwargs.get(\"num_beams\", CFG[\"GEN_BEAMS\"])\n\n    if \"length_penalty\" in gen_kwargs:\n        trainer.model.generation_config.length_penalty = gen_kwargs[\"length_penalty\"]\n    if \"repetition_penalty\" in gen_kwargs:\n        trainer.model.generation_config.repetition_penalty = gen_kwargs[\"repetition_penalty\"]\n    if \"temperature\" in gen_kwargs:\n        trainer.model.generation_config.temperature = gen_kwargs[\"temperature\"]\n\n    out = trainer.evaluate(eval_dataset=ds)\n\n    trainer.args.generation_num_beams = old_beams\n    trainer.model.generation_config.num_beams = CFG[\"GEN_BEAMS\"]\n    return out\n\ntuning_results = []\nif CFG[\"USE_DECODE_TUNING\"]:\n    beams = [1, 3, 5, 8]\n    length_pen = [0.8, 1.0, 1.2]\n    rep_pen = [1.0, 1.1]\n    temps = [0.0, 0.2]\n\n    for b, lp, rp, t in product(beams, length_pen, rep_pen, temps):\n        gen_kwargs = {\"num_beams\": b, \"length_penalty\": lp, \"repetition_penalty\": rp, \"temperature\": t}\n        m = eval_with_gen_kwargs(trainer_b, cv_ds[\"dev\"], gen_kwargs)\n        tuning_results.append({**gen_kwargs, \"wer\": m[\"eval_wer\"], \"cer\": m[\"eval_cer\"]})\n        print(gen_kwargs, \"->\", m[\"eval_wer\"], m[\"eval_cer\"])\n\n    tune_df = pd.DataFrame(tuning_results).sort_values([\"wer\",\"cer\"]).reset_index(drop=True)\n    tune_df.to_csv(\"/kaggle/working/decode_tuning_cv_dev.csv\", index=False)\n    display(tune_df.head(10))\n\n    best = tune_df.iloc[0].to_dict()\n    best_kwargs = {k: best[k] for k in [\"num_beams\",\"length_penalty\",\"repetition_penalty\",\"temperature\"]}\n    print(\"Best decode params:\", best_kwargs)\n\n    best_cv_test = eval_with_gen_kwargs(trainer_b, cv_ds[\"test\"], best_kwargs)\n    pd.DataFrame([{**best_kwargs, \"wer\": best_cv_test[\"eval_wer\"], \"cer\": best_cv_test[\"eval_cer\"]}]).to_csv(\n        \"/kaggle/working/best_decode_cv_test.csv\", index=False\n    )\n    print(\"Best CV test:\", best_cv_test)\nelse:\n    print(\"Decode tuning disabled.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-20T11:30:13.464Z"}},"outputs":[],"execution_count":null},{"id":"b49233db-94f7-456d-9311-348b54f0a2d9","cell_type":"markdown","source":"## 20) Error analysis export (top worst utterances) for supervisor updates","metadata":{}},{"id":"ae6d34c0-e25d-4296-b6f0-72c25eaf9146","cell_type":"code","source":"# Cell 22 — Per-sample error table on a small subset of CV test\n\ndef per_sample_errors(trainer: Seq2SeqTrainer, ds: Dataset, n=400):\n    ds_small = ds.select(range(min(n, len(ds))))\n    preds = trainer.predict(ds_small)\n    pred_ids = preds.predictions\n    label_ids = preds.label_ids\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    rows = []\n    for i, (p, r) in enumerate(zip(pred_str, label_str)):\n        pn, rn = normalize_text(p), normalize_text(r)\n        rows.append({\n            \"idx\": i,\n            \"ref\": rn,\n            \"hyp\": pn,\n            \"wer\": wer(rn, pn),\n            \"cer\": cer(rn, pn),\n            \"speaker_id\": ds_small[i][\"speaker_id\"],\n        })\n    return pd.DataFrame(rows).sort_values(\"wer\", ascending=False)\n\nerr_cv = per_sample_errors(trainer_b, cv_ds[\"test\"], n=500)\nerr_cv.to_csv(\"/kaggle/working/error_analysis_cv_test_top.csv\", index=False)\ndisplay(err_cv.head(25))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-02-20T11:30:13.464Z"}},"outputs":[],"execution_count":null},{"id":"d03b9aa9-3fe3-4ceb-8316-84ff5e0b87a4","cell_type":"markdown","source":"## 21) Weekly update checklist (copy/paste)\n- Stage A (LibriSpeech) dev WER/CER and key settings used (LoRA/8-bit/balanced/SpecAug/filters).  \n- Stage B (Common Voice adaptation) dev + test WER/CER and LS test WER/CER (cross-domain).  \n- Denoise: number of CV samples removed (loss-ranked) and effect on WER/CER.  \n- Decode tuning: best parameters on CV dev and resulting CV test WER/CER.  \n- Error analysis: top failure patterns (numbers, accent, noise, short clips).  \n","metadata":{}}]}