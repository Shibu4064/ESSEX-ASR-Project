{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"932c20c3","cell_type":"markdown","source":"# Cross-Domain ASR (Kaggle P100): Whisper Small.EN — Train on LibriSpeech → Test & Adapt on Local Common Voice v24 en-AU + HPO-lite\n\nThis notebook uses **two domains**:\n\n1) **Source training** on **LibriSpeech** (HF dataset `librispeech_asr`, clean speech)  \n2) **Target evaluation** on your **local Common Voice v24 en-AU**  \n3) **Domain adaptation**: fine-tune on a small local CV train subset  \n4) **HPO-lite**: a small sweep to minimize **WER & CER**\n\nOptimized for Kaggle **GPU P100** and ~**19.5GB** output with checkpoint pruning.\n","metadata":{}},{"id":"25319ff5","cell_type":"code","source":"# 0) Install deps (Kaggle) — pinned to avoid dependency conflicts\n# NOTE: We pin transformers<5 to stay compatible with Kaggle preinstalls (sentence-transformers, etc.)\n# and we remove peft because some Kaggle images ship mismatched peft/transformers versions.\n!pip -q uninstall -y peft || true\n\n!pip -q install \\\n  \"transformers==4.44.2\" \\\n  \"accelerate==0.34.2\" \\\n  \"datasets==2.20.0\" \\\n  \"jiwer==3.0.4\" \\\n  \"soundfile==0.12.1\"\n\nimport os, re, random, json, math, shutil, glob\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchaudio\nimport matplotlib.pyplot as plt\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\nfrom jiwer import wer as jiwer_wer, cer as jiwer_cer\n\nprint(\"torch:\", torch.__version__)\nprint(\"transformers:\", __import__(\"transformers\").__version__)\nprint(\"cuda:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:13:40.028847Z","iopub.execute_input":"2026-02-09T17:13:40.029100Z","iopub.status.idle":"2026-02-09T17:14:34.262007Z","shell.execute_reply.started":"2026-02-09T17:13:40.029069Z","shell.execute_reply":"2026-02-09T17:14:34.261277Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2026-02-09 17:14:21.742206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770657261.900723      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770657261.945776      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770657262.332095      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770657262.332138      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770657262.332141      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770657262.332143      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"torch: 2.8.0+cu126\ntransformers: 4.44.2\ncuda: True\ngpu: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":1},{"id":"322d75a2","cell_type":"code","source":"# 1) Reproducibility + output\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nOUTPUT_DIR = \"/kaggle/working/whisper_ls_to_cv_local_hpo\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nMODEL_NAME = \"openai/whisper-small.en\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:14:38.910708Z","iopub.execute_input":"2026-02-09T17:14:38.911133Z","iopub.status.idle":"2026-02-09T17:14:38.927859Z","shell.execute_reply.started":"2026-02-09T17:14:38.911086Z","shell.execute_reply":"2026-02-09T17:14:38.926637Z"}},"outputs":[],"execution_count":2},{"id":"30d30af1","cell_type":"markdown","source":"## 2) Locate local Common Voice dataset (auto-detection)\n","metadata":{}},{"id":"09d579e7","cell_type":"code","source":"def find_first(patterns, roots=(\"/kaggle/input\", \"/kaggle/working\", \"/kaggle\")):\n    for root in roots:\n        for pat in patterns:\n            hits = glob.glob(os.path.join(root, \"**\", pat), recursive=True)\n            if hits:\n                return hits[0]\n    return None\n\nsplit_csv = find_first([\"commonvoice-v24_en-AU-split.csv\", \"*split*.csv\"])\nmain_csv  = find_first([\"commonvoice-v24_en-AU.csv\", \"*en-AU*.csv\"])\n\nprint(\"Found split CSV:\", split_csv)\nprint(\"Found main  CSV:\", main_csv)\nif main_csv is None and split_csv is None:\n    raise FileNotFoundError(\"Could not find your local Common Voice CSVs.\")\n\nbase_dir = os.path.dirname(split_csv if split_csv else main_csv)\naudio_dir_candidates = [os.path.join(base_dir, \"audio_files\"), os.path.join(base_dir, \"clips\"), os.path.join(base_dir, \"audio\")]\naudio_dir = next((d for d in audio_dir_candidates if os.path.isdir(d)), None)\nprint(\"Base dir:\", base_dir)\nprint(\"Audio dir:\", audio_dir)\nif audio_dir is None:\n    raise FileNotFoundError(\"Could not find audio directory (audio_files/clips/audio).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:14:43.660378Z","iopub.execute_input":"2026-02-09T17:14:43.661014Z","iopub.status.idle":"2026-02-09T17:15:53.437921Z","shell.execute_reply.started":"2026-02-09T17:14:43.660982Z","shell.execute_reply":"2026-02-09T17:15:53.437159Z"}},"outputs":[{"name":"stdout","text":"Found split CSV: /kaggle/input/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU-split.csv\nFound main  CSV: /kaggle/input/mozilla-commonvoice/commonvoice-v24_en-AU/commonvoice-v24_en-AU.csv\nBase dir: /kaggle/input/mozilla-commonvoice/commonvoice-v24_en-AU\nAudio dir: /kaggle/input/mozilla-commonvoice/commonvoice-v24_en-AU/audio_files\n","output_type":"stream"}],"execution_count":3},{"id":"5bd059ed","cell_type":"code","source":"# 3) Load CSVs  ✅ (Corrected: handles BOTH long-split and wide-split CSV formats)\n\ndef read_csv(path):\n    # robust CSV reading (handles different delimiters)\n    try:\n        return pd.read_csv(path)\n    except Exception:\n        return pd.read_csv(path, sep=\";\")\n\ndf_main  = read_csv(main_csv) if main_csv else None\ndf_split = read_csv(split_csv) if split_csv else None\n\nprint(\"main columns:\", list(df_main.columns)[:30] if df_main is not None else None)\nprint(\"split columns:\", list(df_split.columns)[:30] if df_split is not None else None)\n\n# Find transcript column\nTEXT_COL_CANDIDATES = [\"sentence\", \"text\", \"transcript\", \"normalized_text\", \"utterance\"]\ndef pick_col(cols, candidates):\n    for c in candidates:\n        if c in cols:\n            return c\n    return None\n\n# Find audio filename/path column\nAUDIO_COL_CANDIDATES = [\"path\", \"audio_path\", \"filename\", \"file\", \"clip\", \"audio\"]\ndef pick_audio_col(cols):\n    for c in AUDIO_COL_CANDIDATES:\n        if c in cols:\n            return c\n    return None\n\ndef normalize_split_value(x: str) -> str:\n    x = str(x).strip().lower()\n    if x in [\"train\", \"tr\"]:\n        return \"train\"\n    if x in [\"valid\", \"val\", \"validation\", \"dev\", \"devel\"]:\n        return \"val\"\n    if x in [\"test\", \"te\"]:\n        return \"test\"\n    return x\n\n# -------------------------\n# Determine which dataframe to use for splits\n# -------------------------\naudio_col = None  # we may set it inside wide-split parsing\n\nif df_split is not None:\n    df = df_split.copy()\n\n    # Try LONG format split column (values train/val/test in a column)\n    split_col = pick_col(df.columns, [\"split\", \"set\", \"subset\", \"partition\"])\n    if split_col is None:\n        # infer from column values\n        for c in df.columns:\n            vals = set(str(v).lower() for v in df[c].dropna().unique()[:50])\n            if any(v in vals for v in [\"train\", \"validation\", \"valid\", \"dev\", \"test\", \"val\"]):\n                split_col = c\n                break\n\n    # If still none -> try WIDE format where columns are train/val/test and cells contain filenames\n    if split_col is None:\n        wide_split_cols = []\n        for c in df.columns:\n            if normalize_split_value(c) in [\"train\", \"val\", \"test\"]:\n                wide_split_cols.append(c)\n\n        if len(wide_split_cols) > 0:\n            print(\"Detected WIDE split CSV format. Split columns:\", wide_split_cols)\n\n            parts = []\n            for c in wide_split_cols:\n                tmp = df[[c]].dropna().copy()\n                tmp = tmp.rename(columns={c: \"path\"})  # standard filename column\n                tmp[\"split\"] = normalize_split_value(c)\n                parts.append(tmp)\n\n            df = pd.concat(parts, ignore_index=True)\n            split_col = \"split\"\n            audio_col = \"path\"  # now our audio column is 'path'\n\n        else:\n            # No usable split info -> fall back to main CSV (random split later)\n            print(\"WARNING: Split CSV found but no split column detected. Falling back to main CSV.\")\n            df = df_main.copy()\n            split_col = None\n\nelse:\n    df = df_main.copy()\n    split_col = pick_col(df.columns, [\"split\", \"set\", \"subset\", \"partition\"])\n\n# -------------------------\n# Transcript + audio columns\n# -------------------------\ntext_col = pick_col(df.columns, TEXT_COL_CANDIDATES) or (\n    pick_col(df_main.columns, TEXT_COL_CANDIDATES) if df_main is not None else None\n)\nif text_col is None:\n    raise ValueError(\"Could not find transcript column. Try renaming it to 'sentence' or 'text'.\")\n\n# if audio_col not already fixed (wide split case), detect it normally\nif audio_col is None:\n    audio_col = pick_audio_col(df.columns) or (\n        pick_audio_col(df_main.columns) if df_main is not None else None\n    )\nif audio_col is None:\n    raise ValueError(\"Could not find audio path/filename column. Try 'path' or 'filename'.\")\n\nprint(\"Using text column:\", text_col)\nprint(\"Using audio column:\", audio_col)\nprint(\"Using split column:\", split_col)\n\n# Map split values if split column exists\nif split_col is not None and split_col in df.columns:\n    df[split_col] = df[split_col].apply(normalize_split_value)\n\n# If df_split lacks transcript, merge transcript from main by a key (audio_col)\nif df_split is not None and df_main is not None and text_col not in df_split.columns:\n    key = audio_col\n    if key in df_main.columns and key in df.columns:\n        df = df.merge(df_main[[key, text_col]], on=key, how=\"left\")\n\n# -------------------------\n# Build absolute audio paths\n# -------------------------\ndef to_abs_audio_path(x):\n    if pd.isna(x):\n        return None\n    x = str(x)\n    # if already absolute\n    if os.path.isabs(x) and os.path.exists(x):\n        return x\n    # try joining base_dir\n    p1 = os.path.join(base_dir, x)\n    if os.path.exists(p1):\n        return p1\n    # assume filename in audio_dir\n    p2 = os.path.join(audio_dir, os.path.basename(x))\n    if os.path.exists(p2):\n        return p2\n    # handle nested relative like \"clips/xxx.mp3\" or \"audio_files/xxx\"\n    p3 = os.path.join(base_dir, os.path.normpath(x))\n    if os.path.exists(p3):\n        return p3\n    return p2  # last resort\n\ndf[\"audio_path\"] = df[audio_col].apply(to_abs_audio_path)\ndf[\"transcript\"] = df[text_col].astype(str)\n\n# Drop missing audio files\ndf = df[df[\"audio_path\"].apply(lambda p: isinstance(p, str) and os.path.exists(p))].reset_index(drop=True)\n\nprint(\"Rows with existing audio:\", len(df))\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:17:48.130190Z","iopub.execute_input":"2026-02-09T17:17:48.130484Z","iopub.status.idle":"2026-02-09T17:20:40.692520Z","shell.execute_reply.started":"2026-02-09T17:17:48.130442Z","shell.execute_reply":"2026-02-09T17:20:40.691908Z"}},"outputs":[{"name":"stdout","text":"main columns: ['Unnamed: 0', 'client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment', 'duration_ms']\nsplit columns: ['Unnamed: 0', 'client_id', 'path', 'sentence_id', 'sentence', 'sentence_domain', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment', 'duration_ms']\nWARNING: Split CSV found but no split column detected. Falling back to main CSV.\nUsing text column: sentence\nUsing audio column: path\nUsing split column: None\nRows with existing audio: 55673\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                          client_id  \\\n0         182  0621ad6390e651ff1e39b23f74e8d624bd8ba0957d6a19...   \n1         725  18bea6bb076cd9638518d93b4af353c3c329d059789e11...   \n2         885  1db7f61c407a3cdce51e21ae1871ef9ae3ed74d79415ba...   \n3         984  2144a6feb6a906b220cbba00f37b56084bd2d60cce773e...   \n4        1021  22b4048e01f359c779cf9a692f5da7512bfd7ef8c9fff2...   \n\n                           path  \\\n0  common_voice_en_30513358.mp3   \n1  common_voice_en_43618790.mp3   \n2  common_voice_en_21099981.mp3   \n3  common_voice_en_39588772.mp3   \n4  common_voice_en_37211578.mp3   \n\n                                         sentence_id  \\\n0  1f08423cd2153df43fb7c18ddd24edfea7be733046c26d...   \n1  fe3f016c86bea44ab149bc2537de58f16051d8274099a7...   \n2  48e620d96dda152eeec48e28f7b0dde29625591bdc44cd...   \n3  e3d037f2c263d604b0bb1ef837b1fc7ee7f80d5336031c...   \n4  cdfb0356e09201212c123e83ab27e43c48a62b6ce8520d...   \n\n                                            sentence sentence_domain  \\\n0  Princess Vilas herself also contributed person...             NaN   \n1     He has also served in the Chamber of Deputies.             NaN   \n2  Most of his subjects were found in Devon and C...             NaN   \n3  Shots rang out as they fled towards the Austri...             NaN   \n4  The system is based on electromagnetic induction.             NaN   \n\n   up_votes  down_votes       age          gender             accents  \\\n0         4           0     teens  male_masculine  Australian English   \n1         2           0       NaN             NaN  Australian English   \n2         2           0  thirties  male_masculine  Australian English   \n3         2           0       NaN             NaN  Australian English   \n4         2           0       NaN             NaN  Australian English   \n\n   variant locale segment  duration_ms  \\\n0      NaN     en     NaN     7.387500   \n1      NaN     en     NaN     6.307500   \n2      NaN     en     NaN     7.949625   \n3      NaN     en     NaN     5.515500   \n4      NaN     en     NaN     4.055656   \n\n                                          audio_path  \\\n0  /kaggle/input/mozilla-commonvoice/commonvoice-...   \n1  /kaggle/input/mozilla-commonvoice/commonvoice-...   \n2  /kaggle/input/mozilla-commonvoice/commonvoice-...   \n3  /kaggle/input/mozilla-commonvoice/commonvoice-...   \n4  /kaggle/input/mozilla-commonvoice/commonvoice-...   \n\n                                          transcript  \n0  Princess Vilas herself also contributed person...  \n1     He has also served in the Chamber of Deputies.  \n2  Most of his subjects were found in Devon and C...  \n3  Shots rang out as they fled towards the Austri...  \n4  The system is based on electromagnetic induction.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>client_id</th>\n      <th>path</th>\n      <th>sentence_id</th>\n      <th>sentence</th>\n      <th>sentence_domain</th>\n      <th>up_votes</th>\n      <th>down_votes</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>accents</th>\n      <th>variant</th>\n      <th>locale</th>\n      <th>segment</th>\n      <th>duration_ms</th>\n      <th>audio_path</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>182</td>\n      <td>0621ad6390e651ff1e39b23f74e8d624bd8ba0957d6a19...</td>\n      <td>common_voice_en_30513358.mp3</td>\n      <td>1f08423cd2153df43fb7c18ddd24edfea7be733046c26d...</td>\n      <td>Princess Vilas herself also contributed person...</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0</td>\n      <td>teens</td>\n      <td>male_masculine</td>\n      <td>Australian English</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>NaN</td>\n      <td>7.387500</td>\n      <td>/kaggle/input/mozilla-commonvoice/commonvoice-...</td>\n      <td>Princess Vilas herself also contributed person...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>725</td>\n      <td>18bea6bb076cd9638518d93b4af353c3c329d059789e11...</td>\n      <td>common_voice_en_43618790.mp3</td>\n      <td>fe3f016c86bea44ab149bc2537de58f16051d8274099a7...</td>\n      <td>He has also served in the Chamber of Deputies.</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Australian English</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>NaN</td>\n      <td>6.307500</td>\n      <td>/kaggle/input/mozilla-commonvoice/commonvoice-...</td>\n      <td>He has also served in the Chamber of Deputies.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>885</td>\n      <td>1db7f61c407a3cdce51e21ae1871ef9ae3ed74d79415ba...</td>\n      <td>common_voice_en_21099981.mp3</td>\n      <td>48e620d96dda152eeec48e28f7b0dde29625591bdc44cd...</td>\n      <td>Most of his subjects were found in Devon and C...</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>thirties</td>\n      <td>male_masculine</td>\n      <td>Australian English</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>NaN</td>\n      <td>7.949625</td>\n      <td>/kaggle/input/mozilla-commonvoice/commonvoice-...</td>\n      <td>Most of his subjects were found in Devon and C...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>984</td>\n      <td>2144a6feb6a906b220cbba00f37b56084bd2d60cce773e...</td>\n      <td>common_voice_en_39588772.mp3</td>\n      <td>e3d037f2c263d604b0bb1ef837b1fc7ee7f80d5336031c...</td>\n      <td>Shots rang out as they fled towards the Austri...</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Australian English</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>NaN</td>\n      <td>5.515500</td>\n      <td>/kaggle/input/mozilla-commonvoice/commonvoice-...</td>\n      <td>Shots rang out as they fled towards the Austri...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1021</td>\n      <td>22b4048e01f359c779cf9a692f5da7512bfd7ef8c9fff2...</td>\n      <td>common_voice_en_37211578.mp3</td>\n      <td>cdfb0356e09201212c123e83ab27e43c48a62b6ce8520d...</td>\n      <td>The system is based on electromagnetic induction.</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Australian English</td>\n      <td>NaN</td>\n      <td>en</td>\n      <td>NaN</td>\n      <td>4.055656</td>\n      <td>/kaggle/input/mozilla-commonvoice/commonvoice-...</td>\n      <td>The system is based on electromagnetic induction.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"id":"4c4c1f6f","cell_type":"code","source":"# 4) Filter local CV and create subsets for target evaluation/adaptation\nMIN_AUDIO_SEC = 0.2\nMAX_AUDIO_SEC = 15.0\n\ndef get_duration_sec(path: str):\n    try:\n        info = torchaudio.info(path)\n        return info.num_frames / info.sample_rate\n    except Exception:\n        return None\n\ndef normalize_for_metrics(s: str) -> str:\n    s = s.lower()\n    s = re.sub(r\"\\[[^\\]]*\\]\", \" \", s)\n    s = re.sub(r\"[^a-z' ]+\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndf[\"duration\"] = df[\"audio_path\"].apply(get_duration_sec)\ndf = df.dropna(subset=[\"duration\"]).reset_index(drop=True)\ndf = df[(df[\"duration\"] >= MIN_AUDIO_SEC) & (df[\"duration\"] <= MAX_AUDIO_SEC)].reset_index(drop=True)\n\ndf[\"transcript\"] = df[\"transcript\"].astype(str).str.strip()\ndf = df[df[\"transcript\"].str.len() >= 2].reset_index(drop=True)\ndf[\"text_norm\"] = df[\"transcript\"].apply(normalize_for_metrics)\n\nif split_col is not None and split_col in df.columns and df[split_col].isin([\"train\",\"val\",\"test\"]).any():\n    cv_train_df = df[df[split_col] == \"train\"].copy()\n    cv_val_df   = df[df[split_col] == \"val\"].copy()\n    cv_test_df  = df[df[split_col] == \"test\"].copy()\nelse:\n    df_shuf = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n    n = len(df_shuf)\n    n_train = int(0.8*n)\n    n_val = int(0.1*n)\n    cv_train_df = df_shuf.iloc[:n_train].copy()\n    cv_val_df   = df_shuf.iloc[n_train:n_train+n_val].copy()\n    cv_test_df  = df_shuf.iloc[n_train+n_val:].copy()\n\n# Subsample sizes (increase for better results)\nN_CV_TEST = min(800, len(cv_test_df))\nN_CV_FT_TRAIN = min(1500, len(cv_train_df))\nN_CV_FT_VAL = min(300, len(cv_val_df))\n\ncv_test_df = cv_test_df.sample(n=N_CV_TEST, random_state=SEED) if len(cv_test_df) > N_CV_TEST else cv_test_df\ncv_ft_train_df = cv_train_df.sample(n=N_CV_FT_TRAIN, random_state=SEED) if len(cv_train_df) > N_CV_FT_TRAIN else cv_train_df\ncv_ft_val_df = cv_val_df.sample(n=N_CV_FT_VAL, random_state=SEED) if len(cv_val_df) > N_CV_FT_VAL else cv_ft_val_df\n\nprint(\"Local CV subsets:\", len(cv_ft_train_df), len(cv_ft_val_df), len(cv_test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:20:40.693642Z","iopub.execute_input":"2026-02-09T17:20:40.693860Z","iopub.status.idle":"2026-02-09T17:30:46.063270Z","shell.execute_reply.started":"2026-02-09T17:20:40.693840Z","shell.execute_reply":"2026-02-09T17:30:46.062532Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/3868897756.py:7: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  info = torchaudio.info(path)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:20: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:27: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  return AudioMetaData(\n","output_type":"stream"},{"name":"stdout","text":"Local CV subsets: 1500 300 800\n","output_type":"stream"}],"execution_count":6},{"id":"6283fb2d","cell_type":"markdown","source":"## 5) EDA (target test subset)\n","metadata":{}},{"id":"5600eb99","cell_type":"code","source":"from collections import Counter\n\ndef eda_basic(df_, name):\n    print(f\"\\n--- {name} ---\")\n    print(\"N:\", len(df_))\n    print(\"Duration mean:\", round(df_['duration'].mean(), 3), \"p95:\", round(df_['duration'].quantile(0.95), 3))\n    tl = df_[\"text_norm\"].str.len()\n    print(\"Text len mean:\", round(tl.mean(), 2), \"p95:\", round(tl.quantile(0.95), 2))\n    words = Counter(\" \".join(df_[\"text_norm\"].tolist()).split())\n    print(\"Top 10 words:\", words.most_common(10))\n\neda_basic(cv_test_df, \"Local CV TEST (target)\")\nplt.figure()\nplt.hist(cv_test_df[\"duration\"], bins=30)\nplt.title(\"Local CV test: duration (sec)\")\nplt.xlabel(\"sec\"); plt.ylabel(\"count\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:31:20.256808Z","iopub.execute_input":"2026-02-09T17:31:20.257707Z","iopub.status.idle":"2026-02-09T17:31:20.441525Z","shell.execute_reply.started":"2026-02-09T17:31:20.257660Z","shell.execute_reply":"2026-02-09T17:31:20.440952Z"}},"outputs":[{"name":"stdout","text":"\n--- Local CV TEST (target) ---\nN: 800\nDuration mean: 5.063 p95: 8.09\nText len mean: 55.36 p95: 91.0\nTop 10 words: [('the', 556), ('a', 190), ('of', 189), ('to', 177), ('and', 162), ('is', 159), ('in', 157), ('was', 104), ('he', 85), ('it', 82)]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"id":"3eca0380","cell_type":"markdown","source":"## 6) Audio preprocessing + torch datasets\n","metadata":{}},{"id":"0607d945","cell_type":"code","source":"_resamplers = {}\n\ndef trim_silence(wave: torch.Tensor, thr: float = 0.01):\n    if wave.numel() < 10:\n        return wave\n    mask = wave.abs() > thr\n    if mask.any():\n        i0 = int(mask.argmax())\n        i1 = int(len(mask) - mask.flip(0).argmax())\n        return wave[i0:i1]\n    return wave\n\ndef load_audio_16k_mono(path: str, do_trim: bool = False):\n    wav, sr = torchaudio.load(path)\n    if wav.shape[0] > 1:\n        wav = wav.mean(dim=0, keepdim=True)\n    wav = wav.squeeze(0)\n    if sr != 16000:\n        key = (sr, 16000)\n        if key not in _resamplers:\n            _resamplers[key] = torchaudio.transforms.Resample(sr, 16000)\n        wav = _resamplers[key](wav)\n    if do_trim:\n        wav = trim_silence(wav)\n    return wav\n\nclass ASRDataset(torch.utils.data.Dataset):\n    def __init__(self, df_: pd.DataFrame):\n        self.df = df_.reset_index(drop=True)\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        return {\"audio_path\": r.audio_path, \"text\": r.transcript, \"text_norm\": r.text_norm}\n\ncv_test_ds = ASRDataset(cv_test_df)\ncv_ft_train_ds = ASRDataset(cv_ft_train_df)\ncv_ft_val_ds = ASRDataset(cv_ft_val_df)\n\nlen(cv_test_ds), len(cv_ft_train_ds), len(cv_ft_val_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:31:26.220953Z","iopub.execute_input":"2026-02-09T17:31:26.221650Z","iopub.status.idle":"2026-02-09T17:31:26.233787Z","shell.execute_reply.started":"2026-02-09T17:31:26.221617Z","shell.execute_reply":"2026-02-09T17:31:26.233111Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(800, 1500, 300)"},"metadata":{}}],"execution_count":8},{"id":"828990a5","cell_type":"markdown","source":"## 7) Load LibriSpeech (streaming) and sample subsets\n","metadata":{}},{"id":"f525db19-2374-4534-827e-69d791733d50","cell_type":"code","source":"# 7) Load LibriSpeech from LOCAL Kaggle folder (no HF streaming, no load_dataset)\n\nimport os, random\nfrom pathlib import Path\nimport pandas as pd\nimport torchaudio\n\n# If not already defined above\n# MIN_AUDIO_SEC = 0.2\n# def get_duration_sec(path: str):\n#     try:\n#         info = torchaudio.info(path)\n#         return info.num_frames / info.sample_rate\n#     except Exception:\n#         return None\n#\n# def normalize_for_metrics(s: str) -> str:\n#     s = s.lower()\n#     s = re.sub(r\"\\[[^\\]]*\\]\", \" \", s)\n#     s = re.sub(r\"[^a-z' ]+\", \" \", s)\n#     s = re.sub(r\"\\s+\", \" \", s).strip()\n#     return s\n\n# --- auto-detect LibriSpeech root inside /kaggle/input ---\ndef find_librispeech_root():\n    candidates = []\n    for base in [\"/kaggle/input\", \"/kaggle/working\"]:\n        base_p = Path(base)\n        if not base_p.exists():\n            continue\n        for p in base_p.rglob(\"*\"):\n            if p.is_dir() and (\n                (p / \"dev-clean\").exists()\n                or any((p / x).exists() for x in [\"train-clean-100\", \"train-clean-360\", \"train-other-500\"])\n            ):\n                # likely root if it contains LibriSpeech split dirs\n                if (p / \"dev-clean\").exists() and (\n                    (p / \"train-clean-360\").exists() or (p / \"train-clean-100\").exists()\n                ):\n                    candidates.append(p)\n    # pick the shortest path (closest to root)\n    candidates = sorted(candidates, key=lambda x: len(str(x)))\n    return str(candidates[0]) if candidates else None\n\nLIBRISPEECH_ROOT = find_librispeech_root()\nprint(\"Detected LIBRISPEECH_ROOT:\", LIBRISPEECH_ROOT)\nif LIBRISPEECH_ROOT is None:\n    raise FileNotFoundError(\n        \"Could not auto-detect LibriSpeech root. \"\n        \"Set LIBRISPEECH_ROOT manually to the folder that contains dev-clean/train-clean-360/test-clean.\"\n    )\n\n# Choose which splits to use (based on what you said you have)\nLS_TRAIN_DIRS = [\"train-clean-360\"]  # you have this\nLS_VAL_DIRS   = [\"dev-clean\"]        # you have this\nLS_TEST_DIRS  = [\"test-clean\"]       # you have this\n\n# Subset sizes (keep modest for Kaggle P100 speed; increase gradually)\nN_LS_TRAIN = 8000\nN_LS_VAL   = 500\nN_LS_TEST  = 500\n\nMAX_AUDIO_SEC_LS = 15.0  # filter long clips\nMIN_AUDIO_SEC_LS = MIN_AUDIO_SEC\n\ndef iter_librispeech_rows(split_dir: Path):\n    \"\"\"\n    LibriSpeech structure:\n      split_dir/**/**/**.trans.txt\n    Each .trans.txt line:\n      <utt_id> <transcription...>\n    Audio path:\n      same folder / <utt_id>.flac\n    \"\"\"\n    trans_files = list(split_dir.rglob(\"*.trans.txt\"))\n    random.shuffle(trans_files)\n    for tf in trans_files:\n        try:\n            with open(tf, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    parts = line.split()\n                    utt_id = parts[0]\n                    text = \" \".join(parts[1:])\n                    audio_path = tf.parent / f\"{utt_id}.flac\"\n                    if audio_path.exists():\n                        yield str(audio_path), text\n        except Exception:\n            continue\n\ndef build_librispeech_subset(split_names, n_max):\n    rows = []\n    for name in split_names:\n        split_dir = Path(LIBRISPEECH_ROOT) / name\n        if not split_dir.exists():\n            print(f\"WARNING: Missing split folder: {split_dir}\")\n            continue\n\n        for audio_path, text in iter_librispeech_rows(split_dir):\n            if len(text.strip()) < 2:\n                continue\n            dur = get_duration_sec(audio_path)\n            if dur is None:\n                continue\n            if dur < MIN_AUDIO_SEC_LS or dur > MAX_AUDIO_SEC_LS:\n                continue\n\n            rows.append({\n                \"audio_path\": audio_path,\n                \"transcript\": text,\n                \"text_norm\": normalize_for_metrics(text),\n                \"duration\": dur\n            })\n\n            if len(rows) >= n_max:\n                return pd.DataFrame(rows)\n    return pd.DataFrame(rows)\n\nls_train_df = build_librispeech_subset(LS_TRAIN_DIRS, N_LS_TRAIN)\nls_val_df   = build_librispeech_subset(LS_VAL_DIRS,   N_LS_VAL)\nls_test_df  = build_librispeech_subset(LS_TEST_DIRS,  N_LS_TEST)\n\nprint(\"LibriSpeech subsets:\", ls_train_df.shape, ls_val_df.shape, ls_test_df.shape)\ndisplay(ls_train_df.head())\n\n# Wrap into your existing torch Dataset class\nls_train_ds = ASRDataset(ls_train_df)\nls_val_ds   = ASRDataset(ls_val_df)\nls_test_ds  = ASRDataset(ls_test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:40:46.931537Z","iopub.execute_input":"2026-02-09T17:40:46.932169Z","iopub.status.idle":"2026-02-09T17:50:39.323193Z","shell.execute_reply.started":"2026-02-09T17:40:46.932138Z","shell.execute_reply":"2026-02-09T17:50:39.322539Z"}},"outputs":[{"name":"stdout","text":"Detected LIBRISPEECH_ROOT: /kaggle/input/librispeech-asr-corpus\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3868897756.py:7: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  info = torchaudio.info(path)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:20: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:27: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  return AudioMetaData(\n","output_type":"stream"},{"name":"stdout","text":"LibriSpeech subsets: (8000, 4) (500, 4) (500, 4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                          audio_path  \\\n0  /kaggle/input/librispeech-asr-corpus/train-cle...   \n1  /kaggle/input/librispeech-asr-corpus/train-cle...   \n2  /kaggle/input/librispeech-asr-corpus/train-cle...   \n3  /kaggle/input/librispeech-asr-corpus/train-cle...   \n4  /kaggle/input/librispeech-asr-corpus/train-cle...   \n\n                                          transcript  \\\n0  THE READER PERHAPS HAS NOT FORGOTTEN WHAT WAS ...   \n1  AND THEIR NATURE IS ALWAYS SIMILAR WE ARE ALL ...   \n2  MAY ALWAYS ATTAIN TO THE DEGREE OF PROFICIENCY...   \n3  HE COMMUNICATES WITH HIS FELLOWS THROUGH THE M...   \n4  THE APTITUDES OF ALL THE INDIVIDUALS THOUGH VE...   \n\n                                           text_norm   duration  \n0  the reader perhaps has not forgotten what was ...   7.870000  \n1  and their nature is always similar we are all ...  14.100000  \n2  may always attain to the degree of proficiency...  13.915000  \n3  he communicates with his fellows through the m...  11.530000  \n4  the aptitudes of all the individuals though ve...   8.599937  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_path</th>\n      <th>transcript</th>\n      <th>text_norm</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/librispeech-asr-corpus/train-cle...</td>\n      <td>THE READER PERHAPS HAS NOT FORGOTTEN WHAT WAS ...</td>\n      <td>the reader perhaps has not forgotten what was ...</td>\n      <td>7.870000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/librispeech-asr-corpus/train-cle...</td>\n      <td>AND THEIR NATURE IS ALWAYS SIMILAR WE ARE ALL ...</td>\n      <td>and their nature is always similar we are all ...</td>\n      <td>14.100000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/librispeech-asr-corpus/train-cle...</td>\n      <td>MAY ALWAYS ATTAIN TO THE DEGREE OF PROFICIENCY...</td>\n      <td>may always attain to the degree of proficiency...</td>\n      <td>13.915000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/librispeech-asr-corpus/train-cle...</td>\n      <td>HE COMMUNICATES WITH HIS FELLOWS THROUGH THE M...</td>\n      <td>he communicates with his fellows through the m...</td>\n      <td>11.530000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/librispeech-asr-corpus/train-cle...</td>\n      <td>THE APTITUDES OF ALL THE INDIVIDUALS THOUGH VE...</td>\n      <td>the aptitudes of all the individuals though ve...</td>\n      <td>8.599937</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"id":"dd5dc584","cell_type":"markdown","source":"## 8) Whisper model + collator + metrics\n","metadata":{}},{"id":"97128dd9","cell_type":"code","source":"processor = WhisperProcessor.from_pretrained(MODEL_NAME)\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n@dataclass\nclass DataCollatorWhisper:\n    processor: WhisperProcessor\n    do_trim: bool = False\n    max_label_length: int = 256\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        audio_arrays, texts = [], []\n        for f in features:\n            wav = load_audio_16k_mono(f[\"audio_path\"], do_trim=self.do_trim)\n            audio_arrays.append(wav.numpy())\n            texts.append(f[\"text\"])\n\n        inputs = self.processor.feature_extractor(audio_arrays, sampling_rate=16000, return_tensors=\"pt\")\n        input_features = inputs[\"input_features\"]\n\n        labels = self.processor.tokenizer(\n            texts, padding=True, truncation=True, max_length=self.max_label_length, return_tensors=\"pt\"\n        )[\"input_ids\"]\n        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n\n        bos = self.processor.tokenizer.bos_token_id\n        if bos is not None and (labels[:, 0] == bos).all():\n            labels = labels[:, 1:]\n\n        return {\"input_features\": input_features, \"labels\": labels}\n\ndata_collator_default = DataCollatorWhisper(processor=processor, do_trim=False, max_label_length=256)\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    if isinstance(pred_ids, tuple):\n        pred_ids = pred_ids[0]\n    label_ids = pred.label_ids.copy()\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    pred_norm = [normalize_for_metrics(s) for s in pred_str]\n    lab_norm  = [normalize_for_metrics(s) for s in label_str]\n\n    return {\"wer_norm\": jiwer_wer(lab_norm, pred_norm), \"cer_norm\": jiwer_cer(lab_norm, pred_norm)}\n\nprint(\"Model ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T17:50:58.087185Z","iopub.execute_input":"2026-02-09T17:50:58.087690Z","iopub.status.idle":"2026-02-09T17:51:09.360653Z","shell.execute_reply.started":"2026-02-09T17:50:58.087658Z","shell.execute_reply":"2026-02-09T17:51:09.359987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdc840050c74f139c4e755b8186332b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ffaf654377468b96221027650b4d63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701367b184f44ece9acba9f9f7b9960f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84efa33357247c5b920a567c5e9e8b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97456197de54df08f744748c3b5e9b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0ca22edfaa46ee93c4cf422435b108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee175718a5f47d9970cc77dad4fc124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304b477223584679861e4807dfb7941d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f91015bbd13426a97f1631518976e06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5cd2b273c84b4db883a808cab53050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e353aa6441134623a281291181d3cb5d"}},"metadata":{}},{"name":"stdout","text":"Model ready.\n","output_type":"stream"}],"execution_count":11},{"id":"9537045e","cell_type":"markdown","source":"## 9) Args + checkpoint pruning\n","metadata":{}},{"id":"5688c380","cell_type":"code","source":"def prune_checkpoints(folder: str, keep_last_n: int = 1):\n    if not os.path.isdir(folder):\n        return\n    cks = [d for d in os.listdir(folder) if d.startswith(\"checkpoint-\")]\n    if len(cks) <= keep_last_n:\n        return\n    cks = sorted(cks, key=lambda x: int(x.split(\"-\")[-1]))\n    for ck in cks[:-keep_last_n]:\n        shutil.rmtree(os.path.join(folder, ck), ignore_errors=True)\n\ndef make_args(run_name: str, lr: float, epochs: int, batch: int, accum: int, eval_steps: int = 200, max_steps: int = -1):\n    out = os.path.join(OUTPUT_DIR, run_name)\n    os.makedirs(out, exist_ok=True)\n    return Seq2SeqTrainingArguments(\n        output_dir=out,\n        per_device_train_batch_size=batch,\n        per_device_eval_batch_size=batch,\n        gradient_accumulation_steps=accum,\n        learning_rate=lr,\n        warmup_steps=100,\n        num_train_epochs=epochs,\n        fp16=True,\n        gradient_checkpointing=True,\n        evaluation_strategy=\"steps\",\n        save_strategy=\"steps\",\n        eval_steps=eval_steps,\n        save_steps=eval_steps,\n        logging_steps=50,\n        save_total_limit=2,\n        predict_with_generate=True,\n        generation_max_length=225,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"wer_norm\",\n        greater_is_better=False,\n        report_to=\"none\",\n        max_steps=max_steps,\n        remove_unused_columns=False,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:09:27.493252Z","iopub.execute_input":"2026-02-09T18:09:27.493868Z","iopub.status.idle":"2026-02-09T18:09:27.501133Z","shell.execute_reply.started":"2026-02-09T18:09:27.493838Z","shell.execute_reply":"2026-02-09T18:09:27.500323Z"}},"outputs":[],"execution_count":14},{"id":"7ab38f84","cell_type":"markdown","source":"## 10) Stage 1 — Train on LibriSpeech\n","metadata":{}},{"id":"d16b797c","cell_type":"code","source":"S1_RUN = \"stage1_librispeech\"\nS1_LR = 1e-5\nS1_EPOCHS = 2\nS1_BATCH = 4\nS1_ACCUM = 4\n\ns1_args = make_args(S1_RUN, lr=S1_LR, epochs=S1_EPOCHS, batch=S1_BATCH, accum=S1_ACCUM, eval_steps=200)\n\ns1_trainer = Seq2SeqTrainer(\n    model=model,\n    args=s1_args,\n    train_dataset=ls_train_ds,\n    eval_dataset=ls_val_ds,\n    data_collator=data_collator_default,\n    tokenizer=processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\ns1_trainer.train()\ns1_trainer.save_model(os.path.join(OUTPUT_DIR, S1_RUN, \"best_model\"))\nprune_checkpoints(os.path.join(OUTPUT_DIR, S1_RUN), keep_last_n=1)\n\ns1_val = s1_trainer.evaluate()\nprint(\"Stage1 LibriSpeech VAL:\", s1_val)\npd.DataFrame([s1_val]).to_csv(os.path.join(OUTPUT_DIR, S1_RUN, \"val_metrics.csv\"), index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:09:32.048832Z","iopub.execute_input":"2026-02-09T18:09:32.049401Z","iopub.status.idle":"2026-02-09T20:04:15.192163Z","shell.execute_reply.started":"2026-02-09T18:09:32.049371Z","shell.execute_reply":"2026-02-09T20:04:15.191532Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 1:51:32, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer Norm</th>\n      <th>Cer Norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.192300</td>\n      <td>0.193808</td>\n      <td>0.040539</td>\n      <td>0.014716</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.144400</td>\n      <td>0.159531</td>\n      <td>0.090433</td>\n      <td>0.065005</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.075800</td>\n      <td>0.147479</td>\n      <td>0.135873</td>\n      <td>0.097169</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.071500</td>\n      <td>0.141929</td>\n      <td>0.113487</td>\n      <td>0.088742</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.069200</td>\n      <td>0.139301</td>\n      <td>0.140216</td>\n      <td>0.118322</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\nThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='325' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 21:58]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Stage1 LibriSpeech VAL: {'eval_loss': 0.19380831718444824, 'eval_wer_norm': 0.040539035527341574, 'eval_cer_norm': 0.01471615812988333, 'eval_runtime': 180.8963, 'eval_samples_per_second': 2.764, 'eval_steps_per_second': 0.691, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":15},{"id":"8582e4a6","cell_type":"markdown","source":"## 11) Stage 2 — Zero-shot on local Common Voice test\n","metadata":{}},{"id":"80dce4c9","cell_type":"code","source":"cv_zero = s1_trainer.evaluate(cv_test_ds)\nprint(\"Stage2 CV ZERO-SHOT TEST:\", cv_zero)\npd.DataFrame([cv_zero]).to_csv(os.path.join(OUTPUT_DIR, S1_RUN, \"cv_zero_shot_test_metrics.csv\"), index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T20:20:05.533448Z","iopub.execute_input":"2026-02-09T20:20:05.534223Z","iopub.status.idle":"2026-02-09T20:23:54.733207Z","shell.execute_reply.started":"2026-02-09T20:20:05.534190Z","shell.execute_reply":"2026-02-09T20:23:54.732632Z"}},"outputs":[{"name":"stdout","text":"Stage2 CV ZERO-SHOT TEST: {'eval_loss': 0.8991472125053406, 'eval_wer_norm': 0.1042708734128511, 'eval_cer_norm': 0.04551817566041996, 'eval_runtime': 229.1898, 'eval_samples_per_second': 3.491, 'eval_steps_per_second': 0.873, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":16},{"id":"705f4249","cell_type":"markdown","source":"## 12) Stage 3 — Fine-tune on local Common Voice\n","metadata":{}},{"id":"f79fc14a","cell_type":"code","source":"base_model_path = os.path.join(OUTPUT_DIR, S1_RUN, \"best_model\")\nmodel = WhisperForConditionalGeneration.from_pretrained(base_model_path)\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable()\nmodel.to(device)\n\nS3_RUN = \"stage3_cv_finetune\"\nS3_LR = 5e-6\nS3_EPOCHS = 1\nS3_BATCH = 4\nS3_ACCUM = 4\n\ns3_args = make_args(S3_RUN, lr=S3_LR, epochs=S3_EPOCHS, batch=S3_BATCH, accum=S3_ACCUM, eval_steps=200)\n\ns3_trainer = Seq2SeqTrainer(\n    model=model,\n    args=s3_args,\n    train_dataset=cv_ft_train_ds,\n    eval_dataset=cv_ft_val_ds,\n    data_collator=data_collator_default,\n    tokenizer=processor.feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\ns3_trainer.train()\ns3_trainer.save_model(os.path.join(OUTPUT_DIR, S3_RUN, \"best_model\"))\nprune_checkpoints(os.path.join(OUTPUT_DIR, S3_RUN), keep_last_n=1)\n\ns3_val = s3_trainer.evaluate()\ns3_test = s3_trainer.evaluate(cv_test_ds)\nprint(\"Stage3 CV VAL:\", s3_val)\nprint(\"Stage3 CV TEST:\", s3_test)\n\npd.DataFrame([s3_val]).to_csv(os.path.join(OUTPUT_DIR, S3_RUN, \"cv_val_metrics.csv\"), index=False)\npd.DataFrame([s3_test]).to_csv(os.path.join(OUTPUT_DIR, S3_RUN, \"cv_test_metrics.csv\"), index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T20:24:20.617737Z","iopub.execute_input":"2026-02-09T20:24:20.618362Z","iopub.status.idle":"2026-02-09T20:38:17.048172Z","shell.execute_reply.started":"2026-02-09T20:24:20.618334Z","shell.execute_reply":"2026-02-09T20:38:17.047416Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [93/93 08:37, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 04:29]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Stage3 CV VAL: {'eval_loss': 0.28303262591362, 'eval_wer_norm': 0.08243851749220645, 'eval_cer_norm': 0.0345372735011553, 'eval_runtime': 86.0162, 'eval_samples_per_second': 3.488, 'eval_steps_per_second': 0.872, 'epoch': 0.992}\nStage3 CV TEST: {'eval_loss': 0.2983036935329437, 'eval_wer_norm': 0.0908041554444017, 'eval_cer_norm': 0.037525400767667645, 'eval_runtime': 224.7611, 'eval_samples_per_second': 3.559, 'eval_steps_per_second': 0.89, 'epoch': 0.992}\n","output_type":"stream"}],"execution_count":17},{"id":"25fa5ba9","cell_type":"markdown","source":"## 13) Stage 4 — HPO-lite sweep\n","metadata":{}},{"id":"ffef89ad","cell_type":"code","source":"def run_one_cfg(cfg, base_model_path: str):\n    m = WhisperForConditionalGeneration.from_pretrained(base_model_path)\n    m.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n    m.config.use_cache = False\n    m.gradient_checkpointing_enable()\n    m.to(device)\n\n    coll = DataCollatorWhisper(processor=processor, do_trim=cfg[\"do_trim\"], max_label_length=cfg[\"max_label_length\"])\n    args = make_args(cfg[\"run_name\"], lr=cfg[\"lr\"], epochs=cfg[\"epochs\"], batch=cfg[\"batch\"], accum=cfg[\"accum\"], eval_steps=cfg.get(\"eval_steps\", 200), max_steps=cfg.get(\"max_steps\",-1))\n\n    tr = Seq2SeqTrainer(\n        model=m,\n        args=args,\n        train_dataset=cv_ft_train_ds,\n        eval_dataset=cv_ft_val_ds,\n        data_collator=coll,\n        tokenizer=processor.feature_extractor,\n        compute_metrics=compute_metrics,\n    )\n    tr.train()\n    tr.save_model(os.path.join(OUTPUT_DIR, cfg[\"run_name\"], \"best_model\"))\n    prune_checkpoints(os.path.join(OUTPUT_DIR, cfg[\"run_name\"]), keep_last_n=1)\n\n    val_m = tr.evaluate()\n    test_m = tr.evaluate(cv_test_ds)\n\n    return {\n        **cfg,\n        \"val_wer\": float(val_m[\"eval_wer_norm\"]),\n        \"val_cer\": float(val_m[\"eval_cer_norm\"]),\n        \"test_wer\": float(test_m[\"eval_wer_norm\"]),\n        \"test_cer\": float(test_m[\"eval_cer_norm\"]),\n    }\n\nHPO_BASE = os.path.join(OUTPUT_DIR, S1_RUN, \"best_model\")\n\nsearch_space = [\n    {\"run_name\":\"hpo_lr5e-6_e1_trim0\", \"lr\":5e-6, \"epochs\":1, \"batch\":4, \"accum\":4, \"do_trim\":False, \"max_label_length\":256},\n    {\"run_name\":\"hpo_lr1e-5_e1_trim0\", \"lr\":1e-5, \"epochs\":1, \"batch\":4, \"accum\":4, \"do_trim\":False, \"max_label_length\":256},\n    {\"run_name\":\"hpo_lr5e-6_e2_trim0\", \"lr\":5e-6, \"epochs\":2, \"batch\":4, \"accum\":4, \"do_trim\":False, \"max_label_length\":256},\n    {\"run_name\":\"hpo_lr5e-6_e1_trim1\", \"lr\":5e-6, \"epochs\":1, \"batch\":4, \"accum\":4, \"do_trim\":True,  \"max_label_length\":256},\n    {\"run_name\":\"hpo_lr5e-6_e1_lbl192\", \"lr\":5e-6, \"epochs\":1, \"batch\":4, \"accum\":4, \"do_trim\":False, \"max_label_length\":192},\n]\n\nresults = []\nfor cfg in search_space:\n    print(\"\\n=== Running\", cfg[\"run_name\"], \"===\")\n    res = run_one_cfg(cfg, HPO_BASE)\n    results.append(res)\n    pd.DataFrame(results).to_csv(os.path.join(OUTPUT_DIR, \"hpo_lite_results.csv\"), index=False)\n\nres_df = pd.DataFrame(results).sort_values([\"val_wer\",\"val_cer\"]).reset_index(drop=True)\nres_df.to_csv(os.path.join(OUTPUT_DIR, \"SPSS_like_HPO_table.csv\"), index=False)\nprint(\"Top configs:\")\nres_df.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T20:38:40.171276Z","iopub.execute_input":"2026-02-09T20:38:40.171592Z","iopub.status.idle":"2026-02-09T21:28:20.660848Z","shell.execute_reply.started":"2026-02-09T20:38:40.171564Z","shell.execute_reply":"2026-02-09T21:28:20.659696Z"}},"outputs":[{"name":"stdout","text":"\n=== Running hpo_lr5e-6_e1_trim0 ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [93/93 08:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 04:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running hpo_lr1e-5_e1_trim0 ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [93/93 08:27, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 04:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running hpo_lr5e-6_e2_trim0 ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [186/186 16:54, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 04:25]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n=== Running hpo_lr5e-6_e1_trim1 ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3100614061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Running\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHPO_BASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hpo_lite_results.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3100614061.py\u001b[0m in \u001b[0;36mrun_one_cfg\u001b[0;34m(cfg, base_model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprune_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_last_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2237\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/376653069.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maudio_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio_16k_mono\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_trim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_trim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0maudio_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2849307543.py\u001b[0m in \u001b[0;36mload_audio_16k_mono\u001b[0;34m(path, do_trim)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resamplers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_trim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_silence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/2849307543.py\u001b[0m in \u001b[0;36mtrim_silence\u001b[0;34m(wave, thr)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: \"argmax_cpu\" not implemented for 'Bool'"],"ename":"NotImplementedError","evalue":"\"argmax_cpu\" not implemented for 'Bool'","output_type":"error"}],"execution_count":18},{"id":"c34fef77","cell_type":"markdown","source":"## 14) Final summary export\n","metadata":{}},{"id":"8deea80b","cell_type":"code","source":"summary = []\n\ndef safe_read(path):\n    return pd.read_csv(path).iloc[0].to_dict() if os.path.exists(path) else None\n\ns1v = safe_read(os.path.join(OUTPUT_DIR, S1_RUN, \"val_metrics.csv\"))\nif s1v: summary.append({\"stage\":\"stage1_librispeech_val\", **s1v})\n\nzs = safe_read(os.path.join(OUTPUT_DIR, S1_RUN, \"cv_zero_shot_test_metrics.csv\"))\nif zs: summary.append({\"stage\":\"stage2_cv_zero_shot_test\", **zs})\n\ns3v = safe_read(os.path.join(OUTPUT_DIR, \"stage3_cv_finetune\", \"cv_val_metrics.csv\"))\nif s3v: summary.append({\"stage\":\"stage3_cv_val\", **s3v})\n\ns3t = safe_read(os.path.join(OUTPUT_DIR, \"stage3_cv_finetune\", \"cv_test_metrics.csv\"))\nif s3t: summary.append({\"stage\":\"stage3_cv_test\", **s3t})\n\nif os.path.exists(os.path.join(OUTPUT_DIR, \"hpo_lite_results.csv\")):\n    hpo_best = pd.read_csv(os.path.join(OUTPUT_DIR, \"hpo_lite_results.csv\")).sort_values([\"val_wer\",\"val_cer\"]).iloc[0].to_dict()\n    summary.append({\"stage\":\"stage4_hpo_best\", **hpo_best})\n\nsummary_df = pd.DataFrame(summary)\nsummary_df.to_csv(os.path.join(OUTPUT_DIR, \"final_summary_metrics.csv\"), index=False)\nsummary_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T21:28:42.333662Z","iopub.execute_input":"2026-02-09T21:28:42.334211Z","iopub.status.idle":"2026-02-09T21:28:42.374645Z","shell.execute_reply.started":"2026-02-09T21:28:42.334182Z","shell.execute_reply":"2026-02-09T21:28:42.373949Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                      stage  eval_loss  eval_wer_norm  eval_cer_norm  \\\n0    stage1_librispeech_val   0.193808       0.040539       0.014716   \n1  stage2_cv_zero_shot_test   0.899147       0.104271       0.045518   \n2             stage3_cv_val   0.283033       0.082439       0.034537   \n3            stage3_cv_test   0.298304       0.090804       0.037525   \n4           stage4_hpo_best        NaN            NaN            NaN   \n\n   eval_runtime  eval_samples_per_second  eval_steps_per_second  epoch  \\\n0      180.8963                    2.764                  0.691  2.000   \n1      229.1898                    3.491                  0.873  2.000   \n2       86.0162                    3.488                  0.872  0.992   \n3      224.7611                    3.559                  0.890  0.992   \n4           NaN                      NaN                    NaN    NaN   \n\n              run_name       lr  epochs  batch  accum do_trim  \\\n0                  NaN      NaN     NaN    NaN    NaN     NaN   \n1                  NaN      NaN     NaN    NaN    NaN     NaN   \n2                  NaN      NaN     NaN    NaN    NaN     NaN   \n3                  NaN      NaN     NaN    NaN    NaN     NaN   \n4  hpo_lr1e-5_e1_trim0  0.00001     1.0    4.0    4.0   False   \n\n   max_label_length   val_wer   val_cer  test_wer  test_cer  \n0               NaN       NaN       NaN       NaN       NaN  \n1               NaN       NaN       NaN       NaN       NaN  \n2               NaN       NaN       NaN       NaN       NaN  \n3               NaN       NaN       NaN       NaN       NaN  \n4             256.0  0.078975  0.033078  0.086957  0.036126  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stage</th>\n      <th>eval_loss</th>\n      <th>eval_wer_norm</th>\n      <th>eval_cer_norm</th>\n      <th>eval_runtime</th>\n      <th>eval_samples_per_second</th>\n      <th>eval_steps_per_second</th>\n      <th>epoch</th>\n      <th>run_name</th>\n      <th>lr</th>\n      <th>epochs</th>\n      <th>batch</th>\n      <th>accum</th>\n      <th>do_trim</th>\n      <th>max_label_length</th>\n      <th>val_wer</th>\n      <th>val_cer</th>\n      <th>test_wer</th>\n      <th>test_cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>stage1_librispeech_val</td>\n      <td>0.193808</td>\n      <td>0.040539</td>\n      <td>0.014716</td>\n      <td>180.8963</td>\n      <td>2.764</td>\n      <td>0.691</td>\n      <td>2.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>stage2_cv_zero_shot_test</td>\n      <td>0.899147</td>\n      <td>0.104271</td>\n      <td>0.045518</td>\n      <td>229.1898</td>\n      <td>3.491</td>\n      <td>0.873</td>\n      <td>2.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stage3_cv_val</td>\n      <td>0.283033</td>\n      <td>0.082439</td>\n      <td>0.034537</td>\n      <td>86.0162</td>\n      <td>3.488</td>\n      <td>0.872</td>\n      <td>0.992</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>stage3_cv_test</td>\n      <td>0.298304</td>\n      <td>0.090804</td>\n      <td>0.037525</td>\n      <td>224.7611</td>\n      <td>3.559</td>\n      <td>0.890</td>\n      <td>0.992</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>stage4_hpo_best</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>hpo_lr1e-5_e1_trim0</td>\n      <td>0.00001</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>False</td>\n      <td>256.0</td>\n      <td>0.078975</td>\n      <td>0.033078</td>\n      <td>0.086957</td>\n      <td>0.036126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"id":"8dad3524-fa60-4865-b922-88c59e13b45c","cell_type":"markdown","source":"# Replacement of the trim_silence function in the cell 6","metadata":{}},{"id":"e920bd44-81e8-4b8d-ad08-b95c50fae8a6","cell_type":"code","source":"def trim_silence(wave: torch.Tensor, thr: float = 0.01):\n    \"\"\"\n    Trim leading/trailing low-amplitude samples.\n    Bool-safe: does NOT use argmax() on Bool tensors.\n    \"\"\"\n    if wave.numel() < 10:\n        return wave\n\n    mask = (wave.abs() > thr)  # bool\n    idx = torch.nonzero(mask, as_tuple=False).flatten()\n\n    if idx.numel() == 0:\n        return wave  # all silence\n\n    i0 = int(idx[0].item())\n    i1 = int(idx[-1].item()) + 1  # inclusive end -> slice end\n    return wave[i0:i1]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}